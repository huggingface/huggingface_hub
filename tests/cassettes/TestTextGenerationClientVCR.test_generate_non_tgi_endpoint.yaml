interactions:
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "stop": [], "seed":
      null, "temperature": null, "top_k": null, "top_p": null, "truncate": null, "typical_p":
      null, "best_of": null, "watermark": false, "details": false, "decoder_input_details":
      false}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '350'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '{"error":"The following `model_kwargs` are not used by the model: [''watermark'',
        ''stop'', ''decoder_input_details''] (note: typos in the generate
        arguments will also show up in this list)","warnings":["There was an inference
        error: The following `model_kwargs` are not used by the model: [''watermark'',
        ''stop'', ''decoder_input_details''] (note: typos in the generate
        arguments will also show up in this list)"]}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:02 GMT
      Transfer-Encoding:
      - chunked
      access-control-allow-credentials:
      - 'true'
      access-control-expose-headers:
      - x-compute-type, x-compute-time
      server:
      - uvicorn
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.003'
      x-compute-type:
      - cpu
      x-request-id:
      - XMk4gR6bC7lYiuNKaa7dP
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 400
      message: Bad Request
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 3 4 5 6 7 8 9 10 11 12"}]'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '46'
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:02 GMT
      access-control-allow-credentials:
      - 'true'
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.215'
      x-compute-type:
      - cache
      x-request-id:
      - Xu4BHkuFIXmDdmVaAw6ZL
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "4 5 6", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 7 8 9 10 11 12 13 14 15 16"}]'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:03 GMT
      Transfer-Encoding:
      - chunked
      access-control-allow-credentials:
      - 'true'
      access-control-expose-headers:
      - x-compute-type, x-compute-time
      server:
      - uvicorn
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-characters:
      - '5'
      x-compute-time:
      - '0.284'
      x-compute-type:
      - cpu
      x-request-id:
      - KD-x8pfpYvdw7n5zT4InB
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 3 4 5 6 7 8 9 10 11 12"}]'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '46'
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:03 GMT
      access-control-allow-credentials:
      - 'true'
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.215'
      x-compute-type:
      - cache
      x-request-id:
      - DXrY2BH9DQPbdWmrH3-gM
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
version: 1
