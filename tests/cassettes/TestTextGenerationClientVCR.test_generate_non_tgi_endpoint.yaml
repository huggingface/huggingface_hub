interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      X-Amzn-Trace-Id:
      - b3842776-c551-4b5f-acc4-5b4727d6245c
    method: GET
    uri: https://huggingface.co/api/models/gpt2
  response:
    body:
      string: '{"_id":"621ffdc036468d709f17434d","id":"gpt2","private":false,"pipeline_tag":"text-generation","library_name":"transformers","tags":["transformers","pytorch","tf","jax","tflite","rust","onnx","safetensors","gpt2","text-generation","exbert","en","doi:10.57967/hf/0039","license:mit","autotrain_compatible","text-generation-inference","endpoints_compatible","region:us"],"downloads":17272637,"likes":2630,"modelId":"gpt2","author":"openai-community","sha":"607a30d783dfa663caf39e06633721c8d4cfcd7e","lastModified":"2024-02-19T10:57:45.000Z","gated":false,"disabled":false,"widgetData":[{"text":"My
        name is Julien and I like to"},{"text":"I like traveling by train because"},{"text":"Paris
        is an amazing place to visit,"},{"text":"Once upon a time,"}],"model-index":null,"config":{"architectures":["GPT2LMHeadModel"],"model_type":"gpt2","tokenizer_config":{}},"cardData":{"language":"en","tags":["exbert"],"license":"mit"},"transformersInfo":{"auto_model":"AutoModelForCausalLM","pipeline_tag":"text-generation","processor":"AutoTokenizer"},"siblings":[{"rfilename":".gitattributes"},{"rfilename":"64-8bits.tflite"},{"rfilename":"64-fp16.tflite"},{"rfilename":"64.tflite"},{"rfilename":"README.md"},{"rfilename":"config.json"},{"rfilename":"flax_model.msgpack"},{"rfilename":"generation_config.json"},{"rfilename":"merges.txt"},{"rfilename":"model.safetensors"},{"rfilename":"onnx/config.json"},{"rfilename":"onnx/decoder_model.onnx"},{"rfilename":"onnx/decoder_model_merged.onnx"},{"rfilename":"onnx/decoder_with_past_model.onnx"},{"rfilename":"onnx/generation_config.json"},{"rfilename":"onnx/merges.txt"},{"rfilename":"onnx/special_tokens_map.json"},{"rfilename":"onnx/tokenizer.json"},{"rfilename":"onnx/tokenizer_config.json"},{"rfilename":"onnx/vocab.json"},{"rfilename":"pytorch_model.bin"},{"rfilename":"rust_model.ot"},{"rfilename":"tf_model.h5"},{"rfilename":"tokenizer.json"},{"rfilename":"tokenizer_config.json"},{"rfilename":"vocab.json"}],"spaces":["microsoft/HuggingGPT","akhaliq/anychat","Gustavosta/MagicPrompt-Stable-Diffusion","optimum/llm-perf-leaderboard","shi-labs/Versatile-Diffusion","yizhangliu/Grounded-Segment-Anything","gunship999/SexyImages","fffiloni/LatentSync","microsoft/Promptist","h2oai/h2ogpt-chatbot","Yntec/ToyWorld","Manmay/tortoise-tts","aliabid94/AutoGPT","jadechoghari/OpenMusic","FunAudioLLM/CosyVoice2-0.5B","eduagarcia/open_pt_llm_leaderboard","h2oai/h2ogpt-chatbot2","wangrongsheng/ChatPaper","Intel/low_bit_open_llm_leaderboard","OFA-Sys/OFA-Image_Caption","llamameta/flux-pro-uncensored","m-ric/beam_search_visualizer","exbert-project/exbert","fffiloni/EchoMimic","OpenMotionLab/MotionGPT","open-llm-leaderboard/blog","Nymbo/Compare-6","BAAI/open_cn_llm_leaderboard","Yntec/PrintingPress","doevent/Stable-Diffusion-prompt-generator","ShiwenNi/ChatReviewer","Uthar/SexyReality","flax-community/image-captioning","BadToBest/EchoMimic","treadon/prompt-fungineer-355M","fffiloni/echomimic-v2","seawolf2357/sd-prompt-gen","TMElyralab/MuseTalk","llamameta/fluxproV2","nateraw/lavila","yizhangliu/Text-to-Image","FrankZxShen/so-vits-svc-models-ba","gsaivinay/open_llm_leaderboard","maxmax20160403/sovits5.0","deepklarity/poster2plot","Yntec/ToyWorldXL","Nick088/Audio-SR","CISCai/gguf-editor","EleutherAI/magma","akhaliq/CLIP_prefix_captioning","OFA-Sys/OFA-Visual_Grounding","phenixrhyder/NSFW-ToyWorld","phenomenon1981/MagicPrompt-Stable-Diffusion","OFA-Sys/OFA-vqa","FireRedTeam/FireRedTTS","aubmindlab/Arabic-NLP","Gustavosta/MagicPrompt-Dalle","SeaLLMs/SeaLLM-Chat","Yntec/blitz_diffusion","OFA-Sys/OFA-Generic_Interface","johko/capdec-image-captioning","hkunlp/Binder","ShiwenNi/ChatResponse","bipin/image2story","Omnibus/Chatbot-Compare","KBaba7/Quant","oceansweep/tldw","LilyF/Generate_Text_and_Audio","TencentARC/ImageConductor","society-ethics/model-card-regulatory-check","Catmeow/AI_story_writing","ethanchern/Anole","hahahafofo/image2text_prompt_generator","ICML2022/OFA","thirdai/BOLT2.5B","John6666/Diffusion80XX4sg","aliabid94/GPT-Golf","FrankZxShen/so-vits-svc-models-pcr","mshukor/UnIVAL","John6666/PrintingPress4","AiActivity/AI-Assistant","sohaibcs1/Image-to-Text-Summary","BoomerangGirl/MagicPrompt-Stable-Diffusion","Hello-SimpleAI/chatgpt-detector-ling","liyucheng/selective_context","lfoppiano/document-qa","agents-course/beam_search_visualizer","sonalkum/GAMA","GTBench/GTBench","Kaludi/Stable-Diffusion-Prompt-Generator_App","llamameta/fast-sd3.5-large","RitaParadaRamos/SmallCapDemo","llizhx/TinyGPT-V","Vikhrmodels/small-shlepa-lb","gsarti/pecore","Abinivesh/Multi-models-prompt-to-image-generation","DemiPoto/TestDifs","Yntec/Image-Models-Test-April-2024","sasha/BiasDetection","architext/Architext_deployed"],"createdAt":"2022-03-02T23:29:04.000Z","safetensors":{"parameters":{"F32":137022720},"total":137022720},"inference":"warm","usedStorage":13632917530}'
    headers:
      Access-Control-Allow-Origin:
      - https://huggingface.co
      Access-Control-Expose-Headers:
      - X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Xet-Access-Token,X-Xet-Token-Expiration,X-Xet-Refresh-Route,X-Xet-Cas-Url,X-Xet-Hash
      Connection:
      - keep-alive
      Content-Length:
      - '4860'
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Fri, 21 Mar 2025 13:47:13 GMT
      ETag:
      - W/"12fc-2GPIq/wDp7eJaYOQ3S2lVIZw9Fc"
      Referrer-Policy:
      - strict-origin-when-cross-origin
      Vary:
      - Origin
      Via:
      - 1.1 bf37fcd05a816a7fa3bda09195cf83b2.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - EB4gKG3Nm_RLhL4jrffPc6ifew4UbhFV8ZaD6faZ3FOtYfaMEtKJwg==
      X-Amz-Cf-Pop:
      - CDG52-P4
      X-Cache:
      - Miss from cloudfront
      X-Powered-By:
      - huggingface-moon
      X-Request-Id:
      - Root=1-67dd6de1-0f85ac736bab3a6a5af8acfb;b3842776-c551-4b5f-acc4-5b4727d6245c
      cross-origin-opener-policy:
      - same-origin
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "stop": [], "seed":
      null, "temperature": null, "top_k": null, "top_p": null, "truncate": null, "typical_p":
      null, "best_of": null, "watermark": false, "details": false, "decoder_input_details":
      false}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '350'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '{"error":"The following `model_kwargs` are not used by the model: [''watermark'',
        ''stop'', ''decoder_input_details''] (note: typos in the generate
        arguments will also show up in this list)","warnings":["There was an inference
        error: The following `model_kwargs` are not used by the model: [''watermark'',
        ''stop'', ''decoder_input_details''] (note: typos in the generate
        arguments will also show up in this list)"]}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:02 GMT
      Transfer-Encoding:
      - chunked
      access-control-allow-credentials:
      - 'true'
      access-control-expose-headers:
      - x-compute-type, x-compute-time
      server:
      - uvicorn
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.003'
      x-compute-type:
      - cpu
      x-request-id:
      - XMk4gR6bC7lYiuNKaa7dP
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 400
      message: Bad Request
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 3 4 5 6 7 8 9 10 11 12"}]'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '46'
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:02 GMT
      access-control-allow-credentials:
      - 'true'
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.215'
      x-compute-type:
      - cache
      x-request-id:
      - Xu4BHkuFIXmDdmVaAw6ZL
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "4 5 6", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 7 8 9 10 11 12 13 14 15 16"}]'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:03 GMT
      Transfer-Encoding:
      - chunked
      access-control-allow-credentials:
      - 'true'
      access-control-expose-headers:
      - x-compute-type, x-compute-time
      server:
      - uvicorn
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-characters:
      - '5'
      x-compute-time:
      - '0.284'
      x-compute-type:
      - cpu
      x-request-id:
      - KD-x8pfpYvdw7n5zT4InB
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
- request:
    body: '{"inputs": "0 1 2", "parameters": {"do_sample": false, "max_new_tokens":
      10, "repetition_penalty": null, "return_full_text": false, "seed": null, "temperature":
      null, "top_k": null, "top_p": null, "truncate": null, "typical_p": null, "best_of":
      null}, "stream": false}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '268'
      Content-Type:
      - application/json
      user-agent:
      - unknown/None; hf_hub/0.16.0.dev0; python/3.10.6; torch/1.12.1; tensorflow/2.11.0;
        fastcore/1.5.23
    method: POST
    uri: https://router.huggingface.co/hf-inference/models/gpt2
  response:
    body:
      string: '[{"generated_text":" 3 4 5 6 7 8 9 10 11 12"}]'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '46'
      Content-Type:
      - application/json
      Date:
      - Tue, 20 Jun 2023 14:33:03 GMT
      access-control-allow-credentials:
      - 'true'
      vary:
      - Origin, Access-Control-Request-Method, Access-Control-Request-Headers
      x-compute-time:
      - '0.215'
      x-compute-type:
      - cache
      x-request-id:
      - DXrY2BH9DQPbdWmrH3-gM
      x-sha:
      - e7da7f221d5bf496a48136c0cd264e630fe9fcc8
    status:
      code: 200
      message: OK
version: 1