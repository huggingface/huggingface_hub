<!--‚ö†Ô∏è Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Uploader des fichiers sur le Hub

La biblioth√®que `huggingface_hub` offre plusieurs options pour uploader vos fichiers sur le Hub. Vous pouvez utiliser ces fonctions ind√©pendamment ou les int√©grer dans votre propre biblioth√®que.

Chaque fois que vous souhaitez uploader des fichiers sur le Hub, vous devez vous connecter √† votre compte Hugging Face. Pour plus de d√©tails sur l'authentification, consultez [cette section](../quick-start#authentication).

## Uploader un fichier

Une fois que vous avez cr√©√© un d√©p√¥t avec [`create_repo`], vous pouvez uploader un fichier vers votre d√©p√¥t en utilisant [`upload_file`].

Sp√©cifiez le chemin du fichier √† uploader, o√π vous souhaitez uploader le fichier dans le d√©p√¥t, et le nom du d√©p√¥t auquel vous souhaitez ajouter le fichier. Vous pouvez optionnellement d√©finir le type de d√©p√¥t comme `dataset`, `model` ou `space` en fonction de votre besoin.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.upload_file(
...     path_or_fileobj="/path/to/local/folder/README.md",
...     path_in_repo="README.md",
...     repo_id="username/test-dataset",
...     repo_type="dataset", # Uploader vers un d√©p√¥t de dataset
... )
```

## Uploader un dossier

Utilisez la fonction [`upload_folder`] pour uploader un dossier local vers un d√©p√¥t existant. Sp√©cifiez le chemin du dossier local
√† uploader, o√π vous souhaitez uploader le dossier dans le d√©p√¥t, et le nom du d√©p√¥t auquel vous souhaitez ajouter le
dossier. Selon votre type de d√©p√¥t, vous devez d√©finir `dataset`, `model` ou `space`. (model par d√©faut)

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()

# Uploader tout le contenu du dossier local vers votre Space distant.
# Par d√©faut, les fichiers sont upload√©s √† la racine du d√©p√¥t
>>> api.upload_folder(
...     folder_path="/path/to/local/space",
...     repo_id="username/my-cool-space",
...     repo_type="space",
... )
```

Par d√©faut, le fichier `.gitignore` sera pris en compte pour savoir quels fichiers doivent √™tre commit√©s ou non : Nous v√©rifions si un fichier `.gitignore` est pr√©sent dans un commit, si il n'y en a pas, nous v√©rifions s'il existe sur le Hub. Veuillez noter que seul un fichier `.gitignore` pr√©sent √† la racine du r√©pertoire sera utilis√©. Nous ne v√©rifions pas les fichiers `.gitignore` dans les sous-r√©pertoires.

Si vous ne souhaitez pas utiliser un fichier `.gitignore` cod√© en dur, vous pouvez utiliser les arguments `allow_patterns` et `ignore_patterns` pour filtrer les fichiers √† uploader. Ces param√®tres acceptent soit un seul motif, soit une liste de motifs. Les motifs sont des wildcards standard (motifs de globbing) comme document√© [ici](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). Si `allow_patterns` et `ignore_patterns` sont tous deux fournis, les deux contraintes s'appliquent.

En plus du fichier `.gitignore` et des motifs allow/ignore, tout dossier `.git/` pr√©sent dans n'importe quel sous-r√©pertoire sera ignor√©.

```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder",
...     path_in_repo="my-dataset/train", # Uploader vers un dossier sp√©cifique
...     repo_id="username/test-dataset",
...     repo_type="dataset",
...     ignore_patterns="**/logs/*.txt", # Ignorer tous les logs texte (fichiers .txt dans le dossier logs)
... )
```

Vous pouvez √©galement utiliser l'argument `delete_patterns` pour sp√©cifier les fichiers que vous souhaitez supprimer du d√©p√¥t dans le m√™me commit.
Cela peut s'av√©rer utile si vous souhaitez nettoyer un dossier distant avant de pousser des fichiers dedans et que vous ne savez pas quels fichiers
existent d√©j√†.

L'exemple ci-dessous uploade le dossier local `./logs` vers le dossier distant `/experiment/logs/`. Seuls les fichiers txt sont upload√©s
mais avant, il y aura une purge de tous les logs pr√©c√©dents sur le d√©p√¥t.
```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder/logs",
...     repo_id="username/trained-model",
...     path_in_repo="experiment/logs/",
...     allow_patterns="*.txt", # Uploader tous les fichiers texte locaux
...     delete_patterns="*.txt", # Supprimer tous les fichiers texte distants avant
... )
```

## Uploader depuis le CLI

Vous pouvez utiliser la commande `hf upload` depuis le terminal pour uploader directement des fichiers sur le Hub. En interne, elle utilise les m√™mes helpers [`upload_file`] et [`upload_folder`] d√©crits ci-dessus.

Vous pouvez uploader soit un seul fichier, soit un dossier entier :

```bash
# Usage:  hf upload [repo_id] [local_path] [path_in_repo]
>>> hf upload Wauplin/my-cool-model ./models/model.safetensors model.safetensors
https://huggingface.co/Wauplin/my-cool-model/blob/main/model.safetensors

>>> hf upload Wauplin/my-cool-model ./models .
https://huggingface.co/Wauplin/my-cool-model/tree/main
```

`local_path` et `path_in_repo` sont optionnels et peuvent √™tre implicitement d√©duits. Si `local_path` n'est pas d√©fini, l'outil v√©rifiera
si un dossier ou fichier local a le m√™me nom que le `repo_id`. Si c'est le cas, son contenu sera upload√©.
Sinon, une exception est lev√©e demandant √† l'utilisateur de d√©finir explicitement `local_path`. Dans tous les cas, si `path_in_repo` n'est pas
d√©fini, les fichiers seront upload√©s √† la racine du d√©p√¥t.

Pour plus de d√©tails sur la commande upload du CLI, veuillez consulter le [guide CLI](./cli#hf-upload).

## Uploader un grand dossier

Dans la plupart des cas, la m√©thode [`upload_folder`] et la commande `hf upload` devraient √™tre les solutions de r√©f√©rence pour uploader des fichiers sur le Hub. Elles garantissent qu'un seul commit sera effectu√©. Elles g√®rent de nombreux cas d'usage et √©chouent explicitement lorsque quelque chose ne va pas. Cependant, lorsqu'il s'agit d'une grande quantit√© de donn√©es, il faut utiliser la m√©thode [`upload_large_folder`] :
- le processus d'upload est divis√© en plusieurs petites t√¢ches (hachage de fichiers, pr√©-upload de ceux-ci et commit). Chaque fois qu'une t√¢che est termin√©e, le r√©sultat est mis en cache localement dans un dossier `./cache/huggingface` √† l'int√©rieur du dossier que vous essayez d'uploader. En faisant cela, il y a la possibilit√© de red√©marrer le processus apr√®s une interruption et de reprendre toutes les t√¢ches.
- le hachage de gros fichiers et leur pr√©-upload b√©n√©ficient  du multi-threading si votre machine le permet.
- Un m√©canisme de nouvelle tentative a √©t√© ajout√© pour r√©essayer chaque t√¢che ind√©pendante ind√©finiment jusqu'√† ce qu'elle r√©ussisse (peu importe s'il s'agit d'une OSError, ConnectionError, PermissionError, etc.). Ce m√©canisme est √† double tranchant. Si des erreurs transitoires se produisent, le processus continuera et r√©essayera. Si des erreurs permanentes se produisent (par exemple permission refus√©e), il r√©essayera ind√©finiment sans r√©soudre la cause premi√®re. (Retry)

Si vous souhaitez plus de d√©tails techniques sur la fa√ßon dont `upload_large_folder` est impl√©ment√©e, veuillez consulter la r√©f√©rence du package [`upload_large_folder`].

Voici comment utiliser [`upload_large_folder`] dans un script. La signature de la m√©thode est tr√®s similaire √† [`upload_folder`] :

```py
>>> api.upload_large_folder(
...     repo_id="HuggingFaceM4/Docmatix",
...     repo_type="dataset",
...     folder_path="/path/to/local/docmatix",
... )
```

Vous verrez la sortie suivante dans votre terminal :
```
Repo created: https://huggingface.co/datasets/HuggingFaceM4/Docmatix
Found 5 candidate files to upload
Recovering from metadata files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 542.66it/s]

---------- 2024-07-22 17:23:17 (0:00:00) ----------
Files:   hashed 5/5 (5.0G/5.0G) | pre-uploaded: 0/5 (0.0/5.0G) | committed: 0/5 (0.0/5.0G) | ignored: 0
Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 5 | committing: 0 | waiting: 11
---------------------------------------------------
```

D'abord, le d√©p√¥t est cr√©√© s'il n'existait pas auparavant. Ensuite, le dossier local est scann√© pour les fichiers √† uploader. Pour chaque fichier, nous essayons de r√©cup√©rer les m√©tadonn√©es (depuis un upload pr√©c√©demment interrompu). √Ä partir de l√†, il est capable de lancer des workers et d'afficher un statut de mise √† jour toutes les 1 minute. Ici, nous pouvons voir que 5 fichiers ont d√©j√† √©t√© hach√©s mais pas pr√©-upload√©s. 5 workers sont en train de pr√©-uploader des fichiers tandis que les 11 autres attendent une t√¢che.

Une ligne de commande est √©galement disponible. Vous pouvez d√©finir le nombre de workers dans la commande en utilisant l'argument `--num-workers` :

```sh
hf upload-large-folder HuggingFaceM4/Docmatix --repo-type=dataset /path/to/local/docmatix --num-workers=16
```

> [!TIP]
> Pour les grands uploads, vous devez d√©finir `repo_type="model"` ou `--repo-type=model` explicitement. Ceci permet d'√©viter d'avoir des donn√©es upload√©es vers un d√©p√¥t avec un mauvais type. Si c'est le cas, vous devrez malheureusement tout re-uploader.

> [!WARNING]
> Bien qu'√©tant beaucoup plus robuste pour uploader de grands dossiers, `upload_large_folder` est plus limit√©e que [`upload_folder`] au niveau des fonctionnalit√©s. En pratique :
> - vous ne pouvez pas d√©finir un `path_in_repo` personnalis√©. Si vous voulez uploader vers un sous-dossier, vous devez d√©finir la structure appropri√©e localement.
> - vous ne pouvez pas d√©finir un `commit_message` et `commit_description` personnalis√©s car plusieurs commits sont cr√©√©s.
> - vous ne pouvez pas supprimer du d√©p√¥t lors de l'upload. (`delete_patterns` n'est pas support√©)
> - vous ne pouvez pas cr√©er une PR directement. Veuillez d'abord cr√©er une PR (depuis l'interface ou en utilisant [`create_pull_request`]) puis commiter dessus en passant `revision`.

### Conseils et astuces pour les grands uploads

Il existe certaines limitations √† conna√Ætre lorsque vous traitez une grande quantit√© de donn√©es dans votre d√©p√¥t.

Consultez notre guide [Limitations et recommandations des d√©p√¥ts](https://huggingface.co/docs/hub/repositories-recommendations) pour appliquer les meilleures pratiques sur la fa√ßon de structurer vos d√©p√¥ts sur le Hub. Passons maintenant √† quelques conseils pratiques pour rendre votre processus d'upload aussi fluide que possible.

- **Commencez petit** : Nous recommandons de commencer avec une petite quantit√© de donn√©es pour tester votre script d'upload. Il est plus facile d'it√©rer sur un script lorsque l'√©chec ne prend que peu de temps.
- **Attendez-vous √† des √©checs** : Streamer de grandes quantit√©s de donn√©es est difficile. Vous ne savez pas ce qui peut arriver, mais il est toujours pr√©f√©rable de consid√©rer que quelque chose √©chouera au moins une fois - peu importe si c'est d√ª √† votre machine, votre connexion ou nos serveurs. Par exemple, si vous pr√©voyez d'uploader un grand nombre de fichiers, il est pr√©f√©rable de garder une trace localement des fichiers que vous avez d√©j√† upload√©s avant d'uploader le prochain lot. Vous √™tes assur√© qu'un fichier LFS qui est d√©j√† commit√© ne sera jamais re-upload√© deux fois, mais le v√©rifier c√¥t√© client peut quand m√™me √©conomiser du temps. C'est ce que [`upload_large_folder`] est disponible.
- **Utilisez `hf_xet`** : cela exploite le nouveau backend de stockage pour le Hub, est √©crit en Rust et est maintenant disponible pour tout le monde. En r√©alit√©, `hf_xet` est d√©j√† activ√© par d√©faut lors de l'utilisation de `huggingface_hub` ! Pour des performances maximales, d√©finissez [`HF_XET_HIGH_PERFORMANCE=1`](../package_reference/environment_variables.md#hf_xet_high_performance) comme variable d'environnement. Sachez que lorsque le mode haute performance est activ√©, l'outil essaiera d'utiliser toute la bande passante et tous les c≈ìurs CPU disponibles.

## Fonctionnalit√©s avanc√©es

Dans la plupart des cas, vous n'aurez pas besoin de plus que [`upload_file`] et [`upload_folder`] pour uploader vos fichiers sur le Hub.
Cependant, `huggingface_hub` poss√®de des fonctionnalit√©s plus avanc√©es pour faciliter l'upload. Jetons-y un coup d'≈ìil !

### Uploads plus rapides

Profitez d'uploads plus rapides gr√¢ce √† `hf_xet`, la liaison Python vers la biblioth√®que [`xet-core`](https://github.com/huggingface/xet-core) qui permet la d√©duplication bas√©e sur les chunks pour des uploads et t√©l√©chargements plus rapides. `hf_xet` s'int√®gre parfaitement avec `huggingface_hub`, mais utilise la biblioth√®que Rust `xet-core` et le stockage Xet au lieu de LFS.

`hf_xet` utilise le syst√®me de stockage Xet, qui d√©compose les fichiers en chunks immuables, stockant des collections de ces chunks (appel√©s blocks ou xorbs) √† distance et les r√©cup√©rant pour r√©-assembler le fichier lorsque demand√©. Lors de l'upload, apr√®s avoir confirm√© que l'utilisateur est autoris√© √† √©crire dans ce d√©p√¥t, `hf_xet` scannera les fichiers, les d√©composant en leurs chunks et collectant ces chunks dans des xorbs (et d√©dupliquant les chunks connus), puis uploadera ces xorbs vers le service d'adressage de contenu Xet (CAS), qui v√©rifiera l'int√©grit√© des xorbs, enregistrera les m√©tadonn√©es des xorbs ainsi que le hash SHA256 LFS (pour supporter la recherche/t√©l√©chargement), et √©crira les xorbs dans le stockage distant.

Pour l'activer, installez simplement la derni√®re version de `huggingface_hub` :

```bash
pip install -U "huggingface_hub"
```

√Ä partir de `huggingface_hub` 0.32.0, `hf_xet` est activ√© par d√©faut.

Toutes les autres APIs `huggingface_hub` continueront √† fonctionner sans aucune modification. Pour en savoir plus sur les avantages du stockage Xet et `hf_xet`, consultez cette [section](https://huggingface.co/docs/hub/xet/index).

**Consid√©rations pour l'upload depuis un Cluster / Syst√®me de fichiers distribu√©**

Lors de l'upload depuis un cluster, les fichiers upload√©s r√©sident souvent sur un syst√®me de fichiers distribu√© ou en r√©seau (NFS, EBS, Lustre, Fsx, etc.). Le stockage Xet va d√©couper ces fichiers en chunks et les √©crire dans des blocs (√©galement appel√©s xorbs) localement, et une fois le bloc termin√©, les uploadera. Pour de meilleures performances lors de l'upload depuis un syst√®me de fichiers distribu√©, assurez-vous de d√©finir [`HF_XET_CACHE`](../package_reference/environment_variables#hfxetcache) vers un r√©pertoire qui est sur un disque local (ex. un disque NVMe ou SSD local). L'emplacement par d√©faut du cache Xet est sous `HF_HOME` √† (`~/.cache/huggingface/xet`) et celui-ci se trouvant dans le r√©pertoire personnel de l'utilisateur est souvent √©galement situ√© sur le syst√®me de fichiers distribu√©.

### Uploads non-bloquants

Dans certains cas, vous souhaitez pousser des donn√©es sans bloquer votre thread principal. Ceci est particuli√®rement utile pour uploader des logs et
des artefacts tout en continuant un entra√Ænement par exemple. Pour ce faire, vous pouvez utiliser l'argument `run_as_future` dans [`upload_file`] et
[`upload_folder`]. Cela retournera un objet [`concurrent.futures.Future`](https://docs.python.org/3/library/concurrent.futures.html#future-objects)
que vous pouvez utiliser pour v√©rifier le statut de l'upload.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> future = api.upload_folder( # Uploader en arri√®re-plan (action non-bloquante)
...     repo_id="username/my-model",
...     folder_path="checkpoints-001",
...     run_as_future=True,
... )
>>> future
Future(...)
>>> future.done()
False
>>> future.result() # Attendre que l'upload soit termin√© (action bloquante)
...
```

> [!TIP]
> Les t√¢ches en arri√®re-plan sont mises en file d'attente lors de l'utilisation de `run_as_future=True`. Cela signifie que vous √™tes assur√© que les t√¢ches seront
> ex√©cut√©es dans le bon ordre.

M√™me si les t√¢ches en arri√®re-plan sont principalement utiles pour uploader des donn√©es/cr√©er des commits, vous pouvez mettre en file d'attente n'importe quelle m√©thode en utilisant
[`run_as_future`]. Par exemple, vous pouvez l'utiliser pour cr√©er un d√©p√¥t puis uploader des donn√©es dessus en arri√®re-plan. L'
argument int√©gr√© `run_as_future` dans les m√©thodes d'upload est juste un alias autour de lui.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.run_as_future(api.create_repo, "username/my-model", exists_ok=True)
Future(...)
>>> api.upload_file(
...     repo_id="username/my-model",
...     path_in_repo="file.txt",
...     path_or_fileobj=b"file content",
...     run_as_future=True,
... )
Future(...)
```

### Uploader un dossier par chunks

[`upload_folder`] facilite l'upload d'un dossier entier sur le Hub. Cependant, pour les grands dossiers (milliers de fichiers ou
centaines de Go), nous recommandons d'utiliser [`upload_large_folder`], qui divise l'upload en plusieurs commits. Consultez la section [Uploader un grand dossier](#uploader-un-grand-dossier) pour plus de d√©tails.

### Uploads programm√©s

Le Hugging Face Hub facilite la sauvegarde et la version des donn√©es. Cependant, il existe certaines limitations lors de la mise √† jour du m√™me fichier des milliers de fois. Par exemple, vous pourriez vouloir sauvegarder les logs d'un processus d'entra√Ænement ou les retours d'utilisateurs sur un Space d√©ploy√©. Dans ces cas, uploader les donn√©es comme un dataset sur le Hub a du sens, mais cela peut √™tre difficile √† faire correctement. La raison principale est que vous ne voulez pas versionner chaque mise √† jour de vos donn√©es car cela rendrait le d√©p√¥t git inutilisable. La classe [`CommitScheduler`] offre une solution √† ce probl√®me.

L'id√©e est d'ex√©cuter une t√¢che en arri√®re-plan qui pousse r√©guli√®rement un dossier local vers le Hub. Supposons que vous ayez un
Space Gradio qui prend en entr√©e du texte et g√©n√®re deux traductions de celui-ci. Ensuite, l'utilisateur peut s√©lectionner sa traduction pr√©f√©r√©e. Pour chaque ex√©cution, vous voulez sauvegarder l'entr√©e, la sortie et la pr√©f√©rence de l'utilisateur pour analyser les r√©sultats. C'est un
cas d'usage parfait pour [`CommitScheduler`] ; vous voulez sauvegarder des donn√©es sur le Hub (potentiellement des millions de retours d'utilisateurs), mais
vous n'avez pas _besoin_ de sauvegarder en temps r√©el chaque entr√©e d'utilisateur. Au lieu de cela, vous pouvez sauvegarder les donn√©es localement dans un fichier JSON et
les uploader toutes les 10 minutes. Par exemple :

```py
>>> import json
>>> import uuid
>>> from pathlib import Path
>>> import gradio as gr
>>> from huggingface_hub import CommitScheduler

# D√©finir le fichier o√π sauvegarder les donn√©es. Utiliser UUID pour s'assurer de ne pas √©craser les donn√©es existantes d'une ex√©cution pr√©c√©dente.
>>> feedback_file = Path("user_feedback/") / f"data_{uuid.uuid4()}.json"
>>> feedback_folder = feedback_file.parent

# Planifier des uploads r√©guliers. Le d√©p√¥t distant et le dossier local sont cr√©√©s s'ils n'existent pas d√©j√†.
>>> scheduler = CommitScheduler(
...     repo_id="report-translation-feedback",
...     repo_type="dataset",
...     folder_path=feedback_folder,
...     path_in_repo="data",
...     every=10,
... )

# D√©finir la fonction qui sera appel√©e lorsque l'utilisateur soumettra son feedback (√† appeler dans Gradio)
>>> def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None:
...     """
...     Ajouter les entr√©es/sorties et le feedback utilisateur √† un fichier JSON Lines en utilisant un verrou de thread pour √©viter les √©critures concurrentes de diff√©rents utilisateurs.
...     """
...     with scheduler.lock:
...         with feedback_file.open("a") as f:
...             f.write(json.dumps({"input": input_text, "output_1": output_1, "output_2": output_2, "user_choice": user_choice}))
...             f.write("\n")

# D√©marrer Gradio
>>> with gr.Blocks() as demo:
>>>     ... # d√©finir la d√©mo Gradio + utiliser `save_feedback`
>>> demo.launch()
```

C'est tout ! Les entr√©es/sorties utilisateur et le feedback seront disponibles comme un dataset sur le Hub. En utilisant un nom de fichier JSON unique, vous √™tes assur√© de ne pas √©craser les donn√©es d'une ex√©cution pr√©c√©dente ou les donn√©es d'autres
Spaces/r√©pliques poussant simultan√©ment vers le m√™me d√©p√¥t.

Pour plus de d√©tails sur le [`CommitScheduler`], voici ce que vous devez savoir :
- **ajout uniquement :**
    Il est suppos√© que vous ne ferez qu'ajouter du contenu au dossier. Supprimer ou √©craser un fichier pourrait corrompre votre d√©p√¥t.
- **historique git** :
    Le scheduler commitera le dossier toutes les `every` minutes. Pour √©viter de polluer trop le d√©p√¥t git, il est
    recommand√© de d√©finir une valeur minimale de 5 minutes. De plus, le scheduler est con√ßu pour √©viter les commits vides. Si aucun
    nouveau contenu n'est d√©tect√© dans le dossier, le commit programm√© est abandonn√©.
- **erreurs :**
    Le scheduler fonctionne comme un thread en arri√®re-plan. Il est d√©marr√© lorsque vous instanciez la classe et ne s'arr√™te jamais. En particulier,
    si une erreur se produit pendant l'upload (exemple : probl√®me de connexion), le scheduler l'ignorera silencieusement et r√©essayera
    au prochain commit programm√©.

#### D√©mo de persistance de Space

Persister les donn√©es d'un Space vers un Dataset sur le Hub est le principal cas d'usage pour [`CommitScheduler`]. Selon le cas
d'usage, vous pourriez vouloir structurer vos donn√©es diff√©remment. La structure doit √™tre robuste aux utilisateurs concurrents et
aux red√©marrages, ce qui implique souvent de g√©n√©rer des UUIDs. En plus de la robustesse, vous devriez uploader des donn√©es dans un format lisible par la biblioth√®que ü§ó. Nous avons cr√©√© un [Space](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver)
qui montre comment sauvegarder plusieurs formats de donn√©es diff√©rents (vous pourriez avoir besoin de l'adapter pour vos propres besoins sp√©cifiques).

#### Uploads personnalis√©s

[`CommitScheduler`] suppose que vos donn√©es sont en ajout uniquement et doivent √™tre upload√©es "telles quelles". Cependant, vous
pourriez vouloir personnaliser la fa√ßon dont les donn√©es sont upload√©es. Vous pouvez le faire en cr√©ant une classe h√©ritant de [`CommitScheduler`]
et en √©crasant la m√©thode `push_to_hub` (n'h√©sitez pas √† l'√©craser comme vous le souhaitez). Vous √™tes assur√© qu'elle sera
appel√©e toutes les `every` minutes dans un thread en arri√®re-plan. Vous n'avez pas √† vous soucier de la concurrence et des erreurs, mais vous
devez faire attention √† d'autres aspects, comme pousser des commits vides ou des donn√©es dupliqu√©es.

Dans l'exemple (simplifi√©) ci-dessous, nous √©crasons `push_to_hub` pour zipper tous les fichiers PNG dans une seule archive afin d'√©viter
de surcharger le d√©p√¥t sur le Hub :

```py
class ZipScheduler(CommitScheduler):
    def push_to_hub(self):
        # 1. Lister les fichiers PNG
          png_files = list(self.folder_path.glob("*.png"))
          if len(png_files) == 0:
              return None  # retourner t√¥t s'il n'y a rien √† commiter

        # 2. Zipper les fichiers png dans une seule archive
        with tempfile.TemporaryDirectory() as tmpdir:
            archive_path = Path(tmpdir) / "train.zip"
            with zipfile.ZipFile(archive_path, "w", zipfile.ZIP_DEFLATED) as zip:
                for png_file in png_files:
                    zip.write(filename=png_file, arcname=png_file.name)

            # 3. Uploader l'archive
            self.api.upload_file(..., path_or_fileobj=archive_path)

        # 4. Supprimer les fichiers png locaux pour √©viter de les re-uploader plus tard
        for png_file in png_files:
            png_file.unlink()
```

Lorsque vous √©crasez `push_to_hub`, vous avez acc√®s aux attributs de [`CommitScheduler`] et en particulier :
- Client [`HfApi`] : `api`
- Param√®tres du dossier : `folder_path` et `path_in_repo`
- Param√®tres du d√©p√¥t : `repo_id`, `repo_type`, `revision`
- Le verrou de thread : `lock`

> [!TIP]
> Pour plus d'exemples de schedulers personnalis√©s, consultez notre [Space de d√©mo](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver)
> contenant diff√©rentes impl√©mentations selon vos cas d'usage.

### create_commit

Les fonctions [`upload_file`] et [`upload_folder`] sont des APIs qui sont g√©n√©ralement pratiques √† utiliser. Nous recommandons
d'essayer ces fonctions en premier. Cependant, si vous voulez travailler au niveau du commit,
vous pouvez utiliser directement la fonction [`create_commit`].

Il existe trois types d'op√©rations support√©s par [`create_commit`] :

- [`CommitOperationAdd`] uploade un fichier sur le Hub. Si le fichier existe d√©j√†, le contenu du fichier est √©cras√©. Cette op√©ration accepte deux arguments :

  - `path_in_repo` : le chemin du d√©p√¥t vers lequel uploader un fichier.
  - `path_or_fileobj` : soit un chemin vers un fichier sur votre syst√®me de fichiers, soit un objet file-like. C'est le contenu du fichier √† uploader sur le Hub.

- [`CommitOperationDelete`] supprime un fichier ou un dossier d'un d√©p√¥t. Cette op√©ration accepte `path_in_repo` comme argument.

- [`CommitOperationCopy`] copie un fichier dans un d√©p√¥t. Cette op√©ration accepte trois arguments :

  - `src_path_in_repo` : le chemin du d√©p√¥t du fichier √† copier.
  - `path_in_repo` : le chemin du d√©p√¥t o√π le fichier doit √™tre copi√©.
  - `src_revision` : optionnel - la r√©vision du fichier √† copier si vous voulez copier un fichier depuis une branche/r√©vision diff√©rente.

Par exemple, si vous voulez uploader deux fichiers et supprimer un fichier dans un d√©p√¥t Hub :

1. Utilisez le `CommitOperation` appropri√© pour ajouter ou supprimer un fichier et pour supprimer un dossier :

```py
>>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete
>>> api = HfApi()
>>> operations = [
...     CommitOperationAdd(path_in_repo="LICENSE.md", path_or_fileobj="~/repo/LICENSE.md"),
...     CommitOperationAdd(path_in_repo="weights.h5", path_or_fileobj="~/repo/weights-final.h5"),
...     CommitOperationDelete(path_in_repo="old-weights.h5"),
...     CommitOperationDelete(path_in_repo="logs/"),
...     CommitOperationCopy(src_path_in_repo="image.png", path_in_repo="duplicate_image.png"),
... ]
```

2. Passez vos op√©rations √† [`create_commit`] :

```py
>>> api.create_commit(
...     repo_id="lysandre/test-model",
...     operations=operations,
...     commit_message="Upload my model weights and license",
... )
```

En plus de [`upload_file`] et [`upload_folder`], les fonctions suivantes utilisent √©galement [`create_commit`] en interne:

- [`delete_file`] supprime un seul fichier d'un d√©p√¥t sur le Hub.
- [`delete_folder`] supprime un dossier entier d'un d√©p√¥t sur le Hub.
- [`metadata_update`] met √† jour les m√©tadonn√©es d'un d√©p√¥t.

Pour des informations plus d√©taill√©es, consultez la r√©f√©rence [`HfApi`].

### Pr√©-uploader les fichiers LFS avant le commit

Dans certains cas, vous pourriez vouloir uploader d'√©normes fichiers vers S3 **avant** de faire l'appel commit. Par exemple, si vous
commitez un dataset en plusieurs shards qui sont g√©n√©r√©s en m√©moire, vous auriez besoin d'uploader les shards un par un
pour √©viter un probl√®me de m√©moire insuffisante. Une solution est d'uploader chaque shard comme un commit s√©par√© sur le d√©p√¥t. Bien qu'√©tant
parfaitement valide, cette solution a l'inconv√©nient de potentiellement salir l'historique git en g√©n√©rant des dizaines de commits.
Pour surmonter ce probl√®me, vous pouvez uploader vos fichiers un par un vers S3 puis cr√©er un seul commit √† la fin. Ceci
est possible en utilisant [`preupload_lfs_files`] en combinaison avec [`create_commit`].

> [!WARNING]
> Ceci est une m√©thode pour utilisateur exp√©riment√©. Utiliser directement [`upload_file`], [`upload_folder`] ou [`create_commit`] au lieu de g√©rer
> la logique de bas niveau de pr√©-upload. Le principal inconv√©nient de
> [`preupload_lfs_files`] est que jusqu'√† ce que le commit soit r√©ellement fait, les fichiers upload√©s ne sont pas accessibles sur le d√©p√¥t sur
> le Hub. Si vous avez une question, n'h√©sitez pas √† nous contacter sur notre Discord ou dans une issue GitHub.

Voici un exemple simple illustrant comment pr√©-uploader des fichiers :

```py
>>> from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo

>>> repo_id = create_repo("test_preupload").repo_id

>>> operations = [] # Liste de tous les objets `CommitOperationAdd` qui seront g√©n√©r√©s
>>> for i in range(5):
...     content = ... # g√©n√©rer du contenu binaire
...     addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content)
...     preupload_lfs_files(repo_id, additions=[addition])
...     operations.append(addition)

>>> # Cr√©er le commit
>>> create_commit(repo_id, operations=operations, commit_message="Commit all shards")
```

D'abord, nous cr√©ons les objets [`CommitOperationAdd`] un par un. Dans un exemple r√©el, ceux-ci contiendraient les
shards g√©n√©r√©s. Chaque fichier est upload√© avant de g√©n√©rer le suivant. Pendant l'√©tape [`preupload_lfs_files`], **l'
objet `CommitOperationAdd` est mut√©**. Vous devriez uniquement l'utiliser pour le passer directement √† [`create_commit`]. La principale
mise √† jour de l'objet est que **le contenu binaire en est retir√©**, ce qui signifie qu'il sera r√©cup√©r√© par le garbage collector si
vous ne conservez pas une autre r√©f√©rence √† celui-ci. Ceci est totalement normal car nous ne voulons pas garder en m√©moire le contenu qui est
d√©j√† upload√©. Enfin, nous cr√©ons le commit en passant toutes les op√©rations √† [`create_commit`]. Vous pouvez passer
des op√©rations suppl√©mentaires (ajouter, supprimer ou copier).
