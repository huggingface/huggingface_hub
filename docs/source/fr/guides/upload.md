<!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Upload des fichiers vers le Hub

Partager vos fichiers et votre travail est un aspect important du Hub. La librairie `huggingface_hub` offre plusieurs options pour upload vos fichiers vers le Hub. Vous pouvez utiliser ces fonction ind√©pendemment ou les int√©grer √† votre librairie, pour rendre l'int√©raction avec le Hub plus pratique pour vos utilisateurs. Ce guide vous montrera comment push des fichiers:

- Sans utiliser Git.
- Qui sont tr√®s volumineux avec [Git LFS](https://git-lfs.github.com/).
- Avec le gestionnaire de contexte des `commit`.
- Avec la fonction [`~Repository.push_to_hub`].

Lorsque vous voulez upload des fichiers sur le HUb, vous devez vous connecter √† votre compte Hugging Face:

- Connectez vous √† votre compte Hugging Face avec la commande suivante:

  ```bash
  huggingface-cli login
  # Ou en utilisant une variable d\'environnement
  huggingface-cli login --token $HUGGINGFACE_TOKEN
  ```

- Sinon, vous pouvez vous connecter par le code en utilisant [`login`] dans un notebook ou un script:

  ```python
  >>> from huggingface_hub import login
  >>> login()
  ```

  Si lanc√© dans un notebook Jupyter ou Colaboratory, [`login`] d√©marera un widget
  depuis lequel vous pouvez rentrer vos token d'authentification Hugging Face. Sinon,
  un message sera affich√© dans le terminal.

  Il est aussi possible de se connecter par le code sans widget en passant directement
  votre token √† la m√©thode [`login`]. Si vous faites ainsi, faites attention lors du
  partage de votre notenook. Une bonne pratique est de charger le token d'un trousseau
  s√©curis√© aulieu de le sauvegarder en clair dans votre notebook.

## Upload un fichier

Une fois que vous avez cr√©√© un d√©p√¥t avec [`create_repo`], vous puovez upload un fichier sur votre d√©p√¥t en utilisant [`upload_file`].

Pr√©cisez le chemin du fichier √† upload, le nom du d√©p√¥t dans lequel vous voulez ajouter le fichier et l'endroit du d√©p√¥t dans lequel vous voulez qu'il soit. Selon votre type de d√©p√¥t, vous pouvez, facultativement d√©finir le type de d√©p√¥t √† `dataset`, `model` ou `space`.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.upload_file(
...     path_or_fileobj="/path/to/local/folder/README.md",
...     path_in_repo="README.md",
...     repo_id="username/test-dataset",
...     repo_type="dataset",
... )
```

## Upload un dossier

Utilisez la fonction [`upload_folder`] pour upload un dossier local vers un d√©p√¥t. Pr√©cisez le chemin du dossier local,
o√π vous voulez que le dossier soit upload dans le d√©p√¥t et le nom du d√©p√¥t dans lequel vous voulez ajouter le dossier. Selon
votre type de d√©p√¥t, vous pouvez facultativement d√©finir le type de d√©p√¥t √† `dataset`, `model` ou `space`.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()

# Upload tout le contenu du fichier local vers votre space distant
# Par d√©faut, les fichiers sont upload √† la racine du d√©p√¥t
>>> api.upload_folder(
...     folder_path="/path/to/local/space",
...     repo_id="username/my-cool-space",
...     repo_type="space",
... )
```

Par d√©faut, le fichier `.gitignore` sera pris en compte pour savoir quels fichiers doivent √™tre commit ou pas. Par d√©faut, nous v√©rifions si un fichier `.gitignore` est pr√©sent dans un commit, s'il n'y en a pas, nous v√©rifions s'il existe sur le Hub. Notez que seul un fichier `.gitignore` pr√©sent √† la racine du chemin sera utilis√©. Nous ne cherchons pas de fichiers `.gitignore` dans les sous-dossiers.

Si vous ne voulez pas utiliser un fichier `.gitignore` cod√© en dur, vous pouvez utiliser les arguments `allow_patterns` et `ignore_patterns` pour filtrer les fichiers √† upload. Ces param√®tres prennent en entr√©e soit un pattern, soit une liste de patterns. Plus d'informations sur ce que sont les patterns [ici](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). Si `allow_patterns` et `ignore_patterns` sont donn√©s, les deux contraintes s'appliquent.

En plus du fichier `.gitignore` et des patterns allow/ignore, n'importe quel dossier `.git/` pr√©sent dans n'import quel sous chemin sera ignor√©.

```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder",
...     path_in_repo="my-dataset/train", # Upload vers un dossier sp√©cifique
...     repo_id="username/test-dataset",
...     repo_type="dataset",
...     ignore_patterns="**/logs/*.txt", # Ignore tous les logs en .txt
... )
```

Vous pouvez aussi utiliser l'argument `delete_patterns` pour pr√©ciser les fichiers que vous voulez supprimer du d√©p√¥t
dans le m√™me commit. Cet argument peut-√™tre utile si voulez nettoyer un fichier distant avant de push vos fichiers dedans
et que vous ne savez pas quels fichiers existent d√©j√†.

L'exemple ci-dessous upload le fichier local `./logs` vers le fichier distant `/experiment/logs/`. Seul les fichiers textuels
sont upload. Avant √ßa, tous les logs pr√©c√©dents sur le d√©p√¥t sont supprim√©s, le tout dans un seul commit.
```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder/logs",
...     repo_id="username/trained-model",
...     path_in_repo="experiment/logs/",
...     allow_patterns="*.txt", # Upload tous les fichiers textes locaux
...     delete_patterns="*.txt", # Supprime tous les fichiers textes distants avant d'upload
... )
```

## Upload depuis le CLI

Vous pouvez utiliser la commande `huggingface-cli upload` depuis le terminal pour upload directement des fichiers vers le Hub. En interneelle utilise aussi les helpers [`upload_file`] et [`upload_folder`] d√©crits ci dessus.

Vous pouvez upload un unique fichier ou un dossier entier:

```bash
# Cas d'usage:  huggingface-cli upload [repo_id] [local_path] [path_in_repo]
>>> huggingface-cli upload Wauplin/my-cool-model ./models/model.safetensors model.safetensors
https://huggingface.co/Wauplin/my-cool-model/blob/main/model.safetensors

>>> huggingface-cli upload Wauplin/my-cool-model ./models .
https://huggingface.co/Wauplin/my-cool-model/tree/main
```

`local_path` et `path_in_repo` sont optionnels et peuvent √™tre d√©termin√©s implicitement. Si `local_path` n'est pas d√©fini,
l'outil v√©rifiera si un dossier local ou un fichier a le m√™me nom que le `repo_id`. Si ce n'est pas le cas, son contenu
sera upload. Sinon, une exception est lev√©e demandant √† l'utilisateur de d√©finir exxplicitement `local_path`. Dans tous
les cas, si `path_in_repo` n'est pas d√©fini, les fichiers sont upload √† la racine du d√©p√¥t.

Pour plus de d√©tails sur la commande uplaod du CLI, consultez le[guide CLI](./cli#huggingface-cli-upload).

## Fonctionnalit√©s avanc√©es

Dans la plupart des cas, vous n'aurez besoin de rien de plus que [`upload_file`] et [`upload_folder`] pour upload
vos fichiers sur le Hub. Cependant, `huggingface_hub` a des fonctionnalit√©s plus avanc√©es pour rendre le processus
plus simple. Regardons les dans la suite de ce guide.


### Uploads non bloquants

Dans certains cas, vous aura envie de push des donn√©es sans blocker votre thread principale. C'est particuli√®rement
utile pour upload des logs des artefacts tout en continuant √† entrainer un mod√®le. Pour ce faire, vous pouvez utiliser
l'argument `run_as_future` dans [`upload_file`] et [`upload_folder`]. La m√©thode renvera un objet
[`concurrent.futures.Future`](https://docs.python.org/3/library/concurrent.futures.html#future-objects) que vous pouvez utiliser
pour v√©rifier le statu de l'upload.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> future = api.upload_folder( # Upload en arri√®re plan (action non bloquante)
...     repo_id="username/my-model",
...     folder_path="checkpoints-001",
...     run_as_future=True,
... )
>>> future
Future(...)
>>> future.done()
False
>>> future.result() # Attend que l'upload soit finie (action bloquante)
...
```

<Tip>

Le t√¢che d'arri√®re plan sont mise dans une queue en utilisat `run_as_future=True`. Ceci signfie que vous √™tes sur que
la t√¢che sera ex√©cut√©e dans le bon ordre.

</Tip>

M√™me si les t√¢ches en arri√®re plan sont la plupart du temps utiles pour upload des donn√©es ou cr√©er des commits, vous
pouvez mettre dans la queue la m√©thode que vous voulez en utilisant [`run_as_future`]. Par exemple, vous pouvez l'utiliser
pour cr√©er un d√©p√¥t puis upload des donn√©es dessus en arri√®re plan. L'argument `run_as_future` dans les m√©thodes d'upload
est juste un alias autour de cette m√©thode.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.run_as_future(api.create_repo, "username/my-model", exists_ok=True)
Future(...)
>>> api.upload_file(
...     repo_id="username/my-model",
...     path_in_repo="file.txt",
...     path_or_fileobj=b"file content",
...     run_as_future=True,
... )
Future(...)
```

### Upload un dossier par morceaux

[`upload_folder`] rend l'upload d'un dossier entier sur le Hub facile. Cependant, pour des fichiers volumineux (des
milliers de fichiers ou des centaines de GB), cette t√¢che peut rester ardue. Si vous avez un dossier avec beaucoup de fichiers,
vous aurez peut-√™tre envie de l'upload en plusieurs commits. Si vous avez une erreur ou des probl√®mes de connection pendant
l'upload, vous n'aurez surement pas envie de tout recommencer √† z√©ro.

Pour upload un dossier en plusieurs commits, passez simplement l'argument `multi_commits=True`. En arri√®re plan,
`huggingface_hub` listera tous les fichiers pour upload/supprimer et d√©couper le tout en plusieurs commits. La
"strat√©gie" (i.e. comment les commits sont s√©par√©s) est bas√©e sur le nombre et la taille des fichiers √† upload. Une
pull request sera ouverte sur le Hub pour push tous les commits. Une fois la pull request pr√™te, les commits sont
regroup√©s en un seul commit. Si le processu est interrompu avant sa fin, vous pouvez relancer votre script pour continuer
l'upload. La pull request cr√©√© sera  automatiquement d√©tect√©e et l'upload continuera l√† o√π il a √©t√© arr√™t√©. Il est
recommand√© d'utiliser l'argument `multi_commits_verbose=True` pour avoir une meilleure compr√©hension de l'upload et
de sont avancement.

L'exemple ci dessous uploadera plusieurs dossiers vers un dataset en plusieurs commits. Une pull request sera cr√©√© sur le
Hub, elle sera merge automatiquement une fois que l'upload est finie. Si vous pr√©f√©rez que la pull request reste ouverte
pour pouvoir faire une review manuelle, utiliser `create_pr=True`.

```py
>>> upload_folder(
...     folder_path="local/checkpoints",
...     repo_id="username/my-dataset",
...     repo_type="dataset",
...     multi_commits=True,
...     multi_commits_verbose=True,
... )
```

Si vous voulez un meilleur controle de la strat√©gie d'upload (i.e. les commits cr√©√©s), vous pouvez consulter les
m√©thodes bas niveau [`plan_multi_commits`] et [`create_commits_on_pr`].

<Tip warning={true}>

`multi_commits` est toujours une fonctionnalit√© exp√©rimentale. Son API et son comportement pourraient changer dans le futur
sans avertissement pr√©alable.

</Tip>

### Uploads planifi√©es

Le Hub Hugging Face rend facile l'enregistrement et le versionning de donn√©es. Cependant, il y a des limitations lorsqu'on met √† jour un m√™me fichier des milliers de fois. Par exemple, vous aurez peut-√™tre envie d'enregistrer les logs d'un processus d'entrainement ou le feedback des utilisateur sur un space d√©ploy√©. Dans ces deux cas, upload la donn√©e en tant que dataset sur le Hub semble logique, mais il peut-√™tre difficile de le faire correctement. La raison principale est que vous ne voulez pas versionner toutes les mises √† jour de vos donn√©e, car cela rendrait le d√©p√¥t git inutilisable. La classe [`CommitScheduler`] offre une solution √† ce probl√®me.

L'id√©e est de faire tourner une t√¢che en arri√®re plan qui va push √† intervalles r√©guliers un dossier local vers le Hub.
Supposons que vous avez un space Gradio qui prend en entr√© du texte et qui g√©n√©re deux traductions. Dans ce cas, l'utilisateur peut s√©lectionner sa traduction pr√©f√©r√©e. Pour chaque traduction, vous voulez enregistrer l'input, output et les pr√©f√©rences de l'uitlisateur pour analyser les r√©sultats.
C'est un cas d'usage parfait pour [`CommitScheduler`]; vous voulez enregistrer des donn√©es sur le Hub (potentiellement des millions
de retour utilisateurs) mais vous n'avez pas besoin d'enregistrer en temps r√©el chaque input de l'utilisateur. Aulieu de √ßa,
vous pouvez enregistrer les donn√©es en local dans un fichier JSON et l'upload toutes les 10 minutes. Par exemple:

```py
>>> import json
>>> import uuid
>>> from pathlib import Path
>>> import gradio as gr
>>> from huggingface_hub import CommitScheduler

# D√©finit le fichier dans lequel il faut enregistrer les donn√©es. On utilise le UUID pour s'assurer de ne pas overwrite des donn√©es existantes d'un feedback pr√©alable
>>> feedback_file = Path("user_feedback/") / f"data_{uuid.uuid4()}.json"
>>> feedback_folder = feedback_file.parent

# Planifie des uploads r√©guliers. Le d√©p√¥t distant et le dossier local sont cr√©√©s s'il n'existent pas d√©j√†
>>> scheduler = CommitScheduler(
...     repo_id="report-translation-feedback",
...     repo_type="dataset",
...     folder_path=feedback_folder,
...     path_in_repo="data",
...     every=10,
... )

# D√©finit la fonction qui sera appel√©e lorsque l'utilisateur enverra son feedback
>>> def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None:
...     """
...     Append input/outputs and user feedback to a JSON Lines file using a thread lock to avoid concurrent writes from different users.
...     """
...     with scheduler.lock:
...         with feedback_file.open("a") as f:
...             f.write(json.dumps({"input": input_text, "output_1": output_1, "output_2": output_2, "user_choice": user_choice}))
...             f.write("\n")

# Lancement de Gradio
>>> with gr.Blocks() as demo:
>>>     ... # D√©finition de la d√©mo Gradio, ne pas oublier d'utiliser `save_feedback`
>>> demo.launch()
```

Et c'est tout! Lesinputs/outputs et feedback des utilisateur seront disponible en tant que dataset sur le Hub. En utilisant un unique nom de fichier JSON, vous √™tes sur que vous n'overwriterez pas de donn√©es qui se pushent en m√™me sur le m√™me d√©p√¥t.

Pour plus de d√©tails sur le [`CommitScheduler`], voici ce que vous devez savoir:
- **append-only:**
    Il est suppos√© que vous ne faites qu'ajouter du contenu au dossier. Vous devez uniquement ajouter des donn√©es √† 
    des fichiers existants ou cr√©er de nouveaux fichier. Supprimer ou overwrite des fichiers pourrait corrompre votre
    d√©p√¥t.
- **historique git:**
    Le planificateur commitera le dossier toutes les `every` minutes. Pour √©viter de polluer le d√©p√¥t git, il est reccomad√©
    de mettre une valeur minimal d'aumoins 5 minutes. Par ailleurs, les planificateur est cr√©√© pour √©viter les commits
    vides. Si aucun nouveau contenu n'est d√©tect√© dans le dossier, le commit planifi√© sera abandonn√©.
- **erreurs:**
    Le planificateur tourne en tant que thread en arri√®re plan. Il commance quand vous instantiez la classe et ne s'arr√™te
    jamais. En particulier, si une erreur arrive pendant l'upload (par exemple un probl√®me de connexion), le planificateur
    ignorera cette erreur et r√©essaiera au prochain commit planifi√©
- **s√©curit√© des threads:**
    Dans la plupart des cas, il est plus s√©curiser de supposer que vous pouvez √©crire dans un fichier sans se soucier des
    fichiers bloqu√©s. Le planificateur de crashera pas et ne sera pas corrumpu si vous √©crivez du contenue sur le dossier
    pendant l'upload. En pratique, il est possible que de telles probl√®mes arrivent pour des applications lourdes. Dans
    ce cas, nous conseillons d'utiliser le lock `scheduler.lock` pour s'assurer que le thread soient s√©curis√©s. Le lock
    est bloqu√©e uniquement lorsque le planificateur scan le dossier √† la recherche de changements, pas lors de l'upload
    de donn√©es. Vous pouvez sans probl√®me supposer que √ßa n'affectera pas l'exp√©rience utilisateur sur votre space.

#### Space persistence demo

Faire persister des donn√©es d'un space vers un dataset sur le Hub est le cas d'usage le plus courant pour [`CommitScheduler`].
Selon les cas d'usages, vous aurez peut-√™tre envie de structurer vos donn√©es diff√©remment. La structure doit √™tre assez robuste
pour g√©rer simultan√©ment la connexion d'un utilisateur et le red√©marrage ce qui implique souvent la g√©n√©ration d'UUIDs.
En plus de la robustesse, vous devez upload des donn√©es dans un format lisible pour les librairies de datasets ü§ó, afin
de pouvoir les r√©uitiliser plus tard. Nous avons cr√©√© un [space](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver)
qui montre comment enregistrer plusieurs formats de donn√©es ddiff√©rents (vous aurez peut-√™tre besoin de l'adapter √† vos
propres besoins).

#### Uploads personnalis√©es

[`CommitScheduler`] suppose que votre donn√©e est append-only. Cependant, vous aurez peut-√™tre besoin de
customiser la mani√®re dont la donn√©e est upload√©e. Vous pouvez faire √ßa en cr√©ant une classe qui h√©rite
de [`CommitScheduler`] et qui overvrite la m√©thode `push_to_hub`. Vous √™tes sur que cette m√©thode
sera appel√©e toutes les `every` minutes dans un thread en arri√®re plan. Vous n'avez pas besoin de vous
occuper des erreurs et des utilisations simultan√©es, mais vous devez faire attention √† d'autres aspects,
tels que les commits vides ou les donn√©es dupliqu√©es.

Dans l'exemple simplifi√© ci dessous, nous faisons un overwrite de `push_to_hub` pour ziper tous les fichiers PNG
dans une unique archive pour √©viter de surcharger le d√©p√¥t sur le Hub:

```py
class ZipScheduler(CommitScheduler):
    def push_to_hub(self):
        # 1. Liste les fichiers PNG
          png_files = list(self.folder_path.glob("*.png"))
          if len(png_files) == 0:
              return None  # return directement si rien √† commit

        # 2. Zip les fichiers PNG dans une unique archive
        with tempfile.TemporaryDirectory() as tmpdir:
            archive_path = Path(tmpdir) / "train.zip"
            with zipfile.ZipFile(archive_path, "w", zipfile.ZIP_DEFLATED) as zip:
                for png_file in png_files:
                    zip.write(filename=png_file, arcname=png_file.name)

            # 3. Upload l'archive
            self.api.upload_file(..., path_or_fileobj=archive_path)

        # 4. Supprime les fichiers PNG locaux pour √©viter de les r√©-upload plus tard
        for png_file in png_files:
            png_file.unlink()
```

Lorsque vous modifier `push_to_hub` en faisant un overwrite, vous avez acc√®s aux attributs de [`CommitScheduler`], plus pr√©cis√©ment:
- Le client [`HfApi`]: `api`
- Les param√®tres du dossier: `folder_path` et `path_in_repo`
- Les param√®tres du d√©p√¥t: `repo_id`, `repo_type` et `revision`
- Le lock du thread `lock`

<Tip>

Pour plus d'exemples de planififcateurs personnalis√©s, consultez notre
[space de d√©mo](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver) contenant diff√©rentes implementations
d√©pendant de votre cas d'usage.

</Tip>

### create_commit

Les fonctions [`upload_file`] et [`upload_folder`] sonr des APIs de haut niveau qui sont g√©n√©ralement assez pratiques √†
utiliser. Il est recommand√© d'essayer ces fonction en premier si vous ne voulez pas travailler √† un plus bas niveau.
Cependant, si vous voulez travailler au niveau du commit, vous pouvez utiliser directement la fonction [`create_commit`].

Il y a trois types d'op√©rations support√©es par [`create_commit`]:

- [`CommitOperationAdd`] upload un fichier vers le Hub. Si le fichier existe d√©j√†, le contenu du fichier seront overwrite. Cette op√©ration accepte deux arguments:
  - `path_in_repo`: le chemin vers le d√©p√¥t sur lequel vous voulez upload un fichier.
  - `path_or_fileobj`: soit le chemin vers un fichier sur votre machine ou un fichier lui m√™me. C'est le contenu du fichier √† upload vers le Hub.

- [`CommitOperationDelete`] supprime un fichier ou un dossier d'un d√©p√¥t. Cette op√©ration accepte `path_in_repo` en argument.

- [`CommitOperationCopy`] copie un fichier d'un d√©p√¥t. Cette op√©ration prend en entr√© trois arguments:

  - `src_path_in_repo`: le chemin vers le d√©p√¥t du fichier √† copier.
  - `path_in_repo`: le chemin vers le d√©p√¥t sur lequel le fichier doit √™tre copi√©.
  - `src_revision`: argument optionnel - la r√©vision du fichier √† copier si vous voulez copier un fichiers d'une branche/version diff√©rente de main.

Par exmeple, si vous voulez upload deux fichiers et supprimer dossier:

1. Utilisez la bonne `CommitOperation` pour ajouter ou supprimer un fichier en supprimer un dossier:

```py
>>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete
>>> api = HfApi()
>>> operations = [
...     CommitOperationAdd(path_in_repo="LICENSE.md", path_or_fileobj="~/repo/LICENSE.md"),
...     CommitOperationAdd(path_in_repo="weights.h5", path_or_fileobj="~/repo/weights-final.h5"),
...     CommitOperationDelete(path_in_repo="old-weights.h5"),
...     CommitOperationDelete(path_in_repo="logs/"),
...     CommitOperationCopy(src_path_in_repo="image.png", path_in_repo="duplicate_image.png"),
... ]
```

2. Passez vos op√©rations √† [`create_commit`]:

```py
>>> api.create_commit(
...     repo_id="lysandre/test-model",
...     operations=operations,
...     commit_message="Upload my model weights and license",
... )
```

En plus d'[`upload_file`] et [`upload_folder`], les fonctions suivante utilisent aussi [`create_commit`] en arri√®re plan:

- [`delete_file`] supprime un fichier d'un d√©p√¥t sur le Hub.
- [`delete_folder`] supprime un dossier d'un d√©p√¥t sur le Hub.
- [`metadata_update`] Met √† jour les m√©tadonn√©es d'un d√©p√¥t.

Pour plus d'informations, consultez la r√©f√©rence [`HfApi`].

### Preupload des fichier LFS avant le commit

Dans certains cas, vous aurez peut-√™tre envie d'upload d'immense fichiers vers S3 **avant** de faire le commit. Par
exemple, si vous commitez un dataset dans plusieurs shards qui sont g√©n√©r√©es in-memory, vous aurez besoin d'upload
les shards une par une pour √©viter de manquer de m√©moire. Une solution est d'upload chaque shard comme commit s√©par√©
sur le d√©p√¥t. Tout en √©tant parfaitement valide, cette solution a le d√©savantage de brouiller l'historique git en
g√©n√©rant de dizaines de commits. Pour √©viter ce probl√®me, vous pouvez upload vos fichier un par un vers S3 puis cr√©er
un seul commit √† la fin. C'est possible en utilisatn [`preupload_lfs_files`] en combinaison avec [`create_commit`].

<Tip warning={true}>

Cette m√©thode est complexe. Utiliser directement [`upload_file`], [`upload_folder`] ou [`create_commit`] aulieu de
g√©rer la logique bas niveau des fichiers qui s'uploadent en avance est la meilleur mani√®re ed faire dans la plupart
des cas. Le probl√®me principal de [`preupload_lfs_files`] est que tant que le commit est fait, les fichiers ne sont pas
accessibles sur le d√©p√¥t du Hub. Si vous avez une question, n'h√©sitez pas √† nous ping sur Discord ou en cr√©ant
une issue GitHub. 

</Tip>

Voici un exemple simple illustrant comme pre-upload des fichiers:

```py
>>> from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo

>>> repo_id = create_repo("test_preupload").repo_id

>>> operations = [] # Liste de toutes les objets `CommitOperationsAdd` qui seront g√©n√©r√©s
>>> for i in range(5):
...     content = ... # g√©n√®re un contenu binaire
...     addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content)
...     preupload_lfs_files(repo_id, additions=[addition])
...     operations.append(addition)

>>> # Cr√©√© un commit
>>> create_commit(repo_id, operations=operations, commit_message="Commit all shards")
```

Premi√®rement, nous cr√©ons les objets [`CommitOperationAdd`] un par un. Dans un vrai exemple, ils contiendraient
les shard g√©n√©r√©es. Chaque fichier est upload√© avant de g√©n√©rer le suivant. Pendant l'√©tape [`preupload_lfs_files`],
**L'objet `CommitoperationAdd` est mut√©**. Vous devez uniquement l'utiliser pour le passer directement √† [`create_commit`].
Le changement principal sur l'objet est **la suppression du contenu binaire**, ce qui signifie que le ramasse miette
le supprimera si vous ne stockez pas une autre r√©f√©rence √† ce contenu. C'est un m√©canisime pr√©vu car nous ne voulons pas
garder en m√©moire le contenu qui est d√©j√† upload. Enfin, nous cr√©ons un commit en passant toutes les op√©rations √†
[`create_commit`]. Vous pouvez passer des op√©rations suppl√©mentaires (add, delete ou copy) qui n'ont pas encore √©t√©
g√©r√©es et elles le seront correctement.

## Quelques astuces pour les uploads volumineux

Il y a des limitations √† connaitre lors de l'utilisation de grandes quantit√©s de donn√©es sur votre d√©p√¥t. √âtant donn√© le d√©lai pour  transf√©rer
la donn√©e, faire un upload pour avoir une erreur √† la fin du processus, que ce soit sur hf.co ou en local, peut √™tre tr√®s frustrant.

Consultez notre guide sur les [limitations des d√©p√¥ts et recommendations](https://huggingface.co/docs/hub/repositories-recommendations) afin de connaitre les bonnes pratiques sur la structuration de d√©p√¥ts sur le Hub. Maintenant, continuons avec des conseils pratiques pour faire en sorte que vos upload fonctionnent de la mani√®re la plus fluide possible.

- **Commencez petit**: Nous vous recommandons de commencer avec une petite quantit√© de donn√©es pour tester votre script
d'upload. Ce sera plus facile d'it√©rer sur une script lorsque les erreur ne prennent que tr√®s peu de temps √† arriver.
- **Attendez vous √† avoir des erreurs**: D√©placer de grandes quantit√©s de don√©es est un vrai challenge. Vous ne savez
pas ce qui peut se passer, mais il est troujours mieux de consid√©rer que quelque chose va malfonctionner aumoins une fois,
que ce soit votre machine, votre connexion, ou nos serveurs. Par exemple, si vous comptez upload un grand nombre de fichiers,
il vaut mieux garder une trace locale des fichiers que vous avez d√©j√† upload avant d'upload le dossier suivant. Normalement
un fichier LFS d√©j√† commit ne sera jamais re-upload√© deux fois mais le v√©rifier c√¥t√© client peut quand m√™me vous faire
gagner du temps.
- **Utilisez `hf_transfer`**: c'est une [librarie bas√©e sur Rust](https://github.com/huggingface/hf_transfer) qui a pour
but d'acc√©l√©rer les upload sur les machines avec une grande bande passante. Pour l'utiliser, vous devez l'installer
(`pip install hf_transfer`) et l'activer en d√©finissant la variable d'environnement `HF_HUB_ENABLE_HF_TRANSFER=1`. Vous
pouvez enfuiste utiliser `huggingface_hub` normalement. Disclaimer: c'est un outil complexe. Il est test√© et pr√™t √† la
mise en production mais n'a pas toutes les fonctionnalit√©s user-friendly telles que la gestion d'erreurs avanc√©e ou les
proxys. Pour plus de d√©tails, consultez cette [section](https://huggingface.co/docs/huggingface_hub/hf_transfer).

<Tip>

Les barres de progression sont support√©es par `hf_transfer` √† partir de la version `0.1.4`. Mettez √† jour (`pip install -U hf_transfer`) si vous comptez activer les uploads rapides.

</Tip>

## (approche historique) Uploadez des fichiers avec Git LFS

Toutes les m√©thodes d√©crites ci-dessus utilisent l'API du Hub pour upload des fichiers. C'est la m√©thode recommand√©e pour
upload des fichiers sur le Hub. Toutesfois, nous fournissons aussi [`Repository`], un wrapper autour de git pour g√©rer
un d√©p√¥t local.

<Tip warning={true}>

Bien que [`Repository`] ne soit pas r√©ellement deprecated, nous recommandons l'utilisation des m√©thodes bas√©es sur
l'HTTP d√©crite ci dessus. Pour plus de d√©tails sur cette recommandation, consultez [ce guide](../concepts/git_vs_http)
qui explique les diff√©rences fondamentales entre les deux approches.

</Tip>

Git LFS g√®re automatiquement des fichiers d'une taille sup√©rieure √† 10MB. Mais pour les fichiers tr√®s larges (>5GB), vous devez installer un agent
de transfert personnalis√© pour Git LFS:

```bash
huggingface-cli lfs-enable-largefiles
```

Vous devez faire cette installation pour chaque d√©p√¥t qui a un fichier de taille sup√©rieure √† 5GB. Une fois install√©, vous pourrez push
des fichiers volumineux.

### Gestionnaire de contexte de commit

Le gestionnaire de contexte de `commit` g√®re quatre des commandes les plus utilis√©es sur Git: pull, add, commit et push. `git-lfs` traque automatiquement n'importe quel fichier d'une taille sup√©rieure √† 10MB. Dans les exemples suivant, le Le gestionnaire de contexte de `commit`:

1. Pull depuis le d√©p√¥t `text-files`.
2. Ajoute un changment fait √† `file.txt`
3. Commit le changement.
4. Push le changement vers le d√©p√¥t `text-files`.

```python
>>> from huggingface_hub import Repository
>>> with Repository(local_dir="text-files", clone_from="<user>/text-files").commit(commit_message="My first file :)"):
...     with open("file.txt", "w+") as f:
...         f.write(json.dumps({"hey": 8}))
```

Voici un autre exemple expliquant comment utiliser le gestionnaire de contexte de `commit` pour sauvegarder et
upload un fichier vers un d√©p√¥t:

```python
>>> import torch
>>> model = torch.nn.Transformer()
>>> with Repository("torch-model", clone_from="<user>/torch-model", token=True).commit(commit_message="My cool model :)"):
...     torch.save(model.state_dict(), "model.pt")
```

D√©finissez `blocking=False` si vous voulez push vous commits de mani√®re asynchrone. Les comportements non bloquants sont utiles quand vous voulez continuer √† faire tourner un script lorsque vous pushez vos commits.

```python
>>> with repo.commit(commit_message="My cool model :)", blocking=False)
```

Vous pouvez v√©rifier le statut de votre push avec la m√©thode `command_queue`:

```python
>>> last_command = repo.command_queue[-1]
>>> last_command.status
```

R√©f√©rez vous √† la table ci dessous pour la liste de statuts possibles:

| Statut   | Description                          |
| -------- | ------------------------------------ |
| -1       |  Le push est en cours                |
| 0        |  Le push s'est fini sans erreurs.    |
| Non-zero |  Il y a eu une erreur.               |

Lorsque vous utilisez `blocking=False`, les commandes sont suivies, et votre script se finira uniquement lorsque toues les push sont finis, m√™me si d'autres erreurs arrivent dans votre script. Voici des commandes utiles pour v√©rifier le statut d'un push:

```python
# Inspecte une erreur
>>> last_command.stderr

# V√©rifie si un push est fini ou en cours
>>> last_command.is_done

# V√©rifie si une commande push a donn√© une erreur
>>> last_command.failed
```

### push_to_hub

la classe [`Repository`] a une fonction [`~Repository.push_to_hub`] pour ajouter des fichiers, faire un commit et les push vers un d√©p√¥t. A la diff√©rence du gestionnaire de contexte de `commit`, vous aurez besoin de pull depuis un d√©p√¥t d'abord avant d'appeler [`~Repository.push_to_hub`].

Par exemple, si vous avez d√©j√† clon√© un d√©p√¥t du Hub, vous pouvez initialiser le `repo` depuis le chemin local:

```python
>>> from huggingface_hub import Repository
>>> repo = Repository(local_dir="path/to/local/repo")
```

Mettez √† jour votre clone local avec [`~Repository.git_pull`] et pushez ensuite votre fichier vers le Hub:

```py
>>> repo.git_pull()
>>> repo.push_to_hub(commit_message="Commit my-awesome-file to the Hub")
```

Cependant si vous n'√™tes pas pr√™t √† push un fichier, vous pouvez utiliser [`~Repository.git_add`] et [`~Repository.git_commit`] pour simplement add et commit votre fichier:

```py
>>> repo.git_add("path/to/file")
>>> repo.git_commit(commit_message="add my first model config file :)")
```

Une fois que vous √™tes pr√™t, push le fichier vers votre d√©p√¥t avec [`~Repository.git_push`]:

```py
>>> repo.git_push()
```
