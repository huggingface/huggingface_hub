<!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Ligne de commande (CLI)

L'interface en ligne de commande `huggingface-cli` vous permet d'interagir directement avec le Hub depuis votre terminal. Vous pouvez cr√©er et g√©rer des d√©p√¥ts, t√©l√©charger et uploader des fichiers, et effectuer d'autres op√©rations directement depuis votre terminal de commande.

## Installation

L'outil CLI `huggingface-cli` est inclus dans le package Python `huggingface_hub` :

```bash
pip install -U "huggingface_hub[cli]"
```

Si vous utilisez `pip install huggingface_hub` sans le composant `[cli]`, seules les commandes basiques sont install√©es par d√©faut. Pour installer toutes les fonctionnalit√©s CLI (par exemple, la gestion du cache, la validation), il est recommand√© d'installer le package avec le composant `[cli]`.

Pour mettre √† jour le package vers la derni√®re version, ex√©cutez :

```bash
pip install -U "huggingface_hub[cli]"
```

Pour v√©rifier que la CLI est correctement install√©e, vous pouvez ex√©cuter :

```bash
huggingface-cli --help
```

Alternative : Installation avec curl (Linux/macOS)

```bash
curl -L https://hf.co/install-cli.sh | sh
```

Alternative : Installation avec `uv`

```bash
uv tool install "huggingface_hub[cli]"
```

Alternative : Installation avec Homebrew (macOS/Linux)

```bash
brew install huggingface-cli
```

> [!TIP]
> Dans toute la documentation, vous verrez des exemples utilisant `huggingface-cli` ou `hf`. Ces deux commandes sont √©quivalentes, `hf` √©tant simplement un alias pour le m√™me outil. Utilisez celui que vous pr√©f√©rez, mais nous utiliserons g√©n√©ralement `hf` car il est plus court.

## hf auth login

Dans de nombreux cas, vous devrez vous connecter avec un compte Hugging Face pour interagir avec le Hub, que ce soit pour t√©l√©charger des d√©p√¥ts priv√©s, uploader des fichiers, cr√©er des PRs, etc. Utilisez la commande suivante dans votre terminal pour vous connecter :

```bash
hf auth login
```

Cette commande vous indiquera si vous √™tes d√©j√† connect√© et vous invitera √† saisir votre jeton d'acc√®s. Vous pouvez cr√©er un jeton d'acc√®s depuis vos [Param√®tres de compte](https://huggingface.co/settings/tokens). Une fois connect√©, le jeton d'acc√®s sera stock√© dans votre r√©pertoire de cache (`~/.cache/huggingface/token` par d√©faut) et sera automatiquement utilis√© lors de l'ex√©cution de toute commande ou script Python appelant `huggingface_hub`.

Vous pouvez √©galement passer votre jeton en utilisant l'option `--token` :

```bash
hf auth login --token YOUR_TOKEN
```

### Se connecter via une variable d'environnement

Vous pouvez d√©finir votre jeton en tant que variable d'environnement `HF_TOKEN`. Cela permet au syst√®me d'authentification de r√©cup√©rer le jeton m√™me sans passer par `hf auth login`. Ceci est particuli√®rement utile pour les serveurs ou les environnements CI/CD qui ne permettent pas l'interaction avec les commandes.

```bash
export HF_TOKEN="YOUR_TOKEN"
```

### Se connecter avec git credentials

Alternativement, vous pouvez vous connecter en utilisant git credentials. Ceci est utile si vous souhaitez acc√©der aux d√©p√¥ts Hugging Face depuis git directement et non via des scripts Python.

```bash
hf auth login --git-credential
```

Par d√©faut, cela configurera git pour utiliser le helper `store` qui stockera vos credentials en texte clair sur votre machine. Si vous pr√©f√©rez utiliser un keyring pour stocker vos credentials de mani√®re s√©curis√©e, utilisez `--git-credential-with-keyring` (n√©cessite `keyring` : `pip install keyring`) :

```bash
hf auth login --git-credential-with-keyring
```

Dans ce cas, git sera configur√© pour utiliser le helper `huggingface` qui interagit avec votre keyring pour stocker et r√©cup√©rer vos credentials. Voir [Git credentials](../package_reference/environment_variables#hfhubgitcredential) pour plus de d√©tails.

Enfin, vous pouvez √©galement ajouter votre jeton directement au git remote :

```bash
# Utilisez votre nom d'utilisateur et le jeton comme mot de passe
git clone https://USER:TOKEN@huggingface.co/my-username/my-model
```

> [!TIP]
> Configurez `--add-to-git-credential` en plus de `--token` pour vous connecter avec un jeton et le stocker dans git en une seule commande.

## hf auth whoami

Si vous souhaitez savoir si vous √™tes connect√©, vous pouvez utiliser `hf auth whoami`. Cette commande ne n√©cessite aucune authentification et n'est donc utile que pour v√©rifier si vous √™tes connect√© ou pour obtenir votre nom d'utilisateur et les organisations auxquelles vous appartenez :

```bash
hf auth whoami
```

Exemple de sortie :

```bash
Wauplin
orgs: huggingface,eu-test,hf-accelerate
```

## hf auth logout

Enfin, vous pouvez vous d√©connecter en utilisant `hf auth logout`. Cette commande supprimera le jeton d'acc√®s de votre cache (`~/.cache/huggingface/token`). Notez que votre jeton pourrait encore √™tre disponible si vous l'avez d√©fini via la variable d'environnement `HF_TOKEN`.

```bash
hf auth logout
```

## hf download

Utilisez la commande `hf download` pour t√©l√©charger des fichiers depuis le Hub. En interne, elle utilise les m√™mes helpers [`hf_hub_download`] et [`snapshot_download`] d√©crits dans le guide [T√©l√©charger](./download). Dans les exemples ci-dessous, nous passerons en revue les cas d'utilisation les plus courants. Pour une liste compl√®te des options disponibles, vous pouvez ex√©cuter :

```bash
hf download --help
```

### T√©l√©charger un fichier unique

Pour t√©l√©charger un fichier unique depuis un d√©p√¥t, utilisez simplement la commande `hf download repo_id filename`.

```bash
hf download gpt2 config.json
```

Par d√©faut, le fichier sera t√©l√©charg√© dans le r√©pertoire de cache d√©fini par la variable d'environnement `HF_HOME`. Cependant, dans la plupart des cas, vous souhaiterez probablement d√©finir o√π le fichier va √™tre t√©l√©charg√©. La mani√®re la plus simple de le faire est d'utiliser l'option `--local-dir`. Le chemin renvoy√© sera alors "human-readable" :

```bash
>>> hf download gpt2 config.json --local-dir=./models/gpt2
./models/gpt2/config.json
```

### T√©l√©charger un d√©p√¥t entier

Dans certains cas, vous souhaiterez simplement t√©l√©charger tous les fichiers d'un d√©p√¥t. Pour ce faire, omettez simplement l'argument `filename` :

```bash
>>> hf download HuggingFaceH4/zephyr-7b-beta
/home/wauplin/.cache/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/snapshots/3bac358730f8806e5c3dc7c7e19eb36e045bf720
```

### T√©l√©charger plusieurs fichiers

Vous pouvez √©galement t√©l√©charger un sous-ensemble de fichiers d'un d√©p√¥t avec un seul appel. Il existe deux mani√®res de le faire. Si vous avez une liste pr√©cise de fichiers √† t√©l√©charger, vous pouvez simplement fournir une liste d'arguments `filename` :

```bash
>>> hf download gpt2 config.json model.safetensors
/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10
```

Cependant, dans la plupart des cas d'utilisation, vous souhaiterez probablement filtrer les fichiers que vous souhaitez t√©l√©charger en utilisant un pattern (par exemple, t√©l√©charger tous les safetensors weights mais pas les sharded PyTorch weights). Vous pouvez le faire en utilisant les options `--include` et `--exclude`. Par exemple, pour t√©l√©charger tous les fichiers JSON et Markdown sauf `vocab.json` :

```bash
>>> hf download gpt2 --include="*.json" --include="*.md" --exclude="vocab.json"
Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 41662.15it/s]
/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10
```

### T√©l√©charger un dataset ou un Space

Les exemples ci-dessus montrent comment t√©l√©charger depuis un d√©p√¥t de mod√®les. Pour t√©l√©charger un dataset ou un Space, utilisez les options `--repo-type=dataset` et `--repo-type=space` :

```bash
# T√©l√©chargez un dataset unique
>>> hf download --repo-type=dataset lhoestq/custom_squad --include="*.json" --local-dir=./datasets/custom_squad
Fetching 9 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 87664.16it/s]
./datasets/custom_squad

# T√©l√©chargez le code d'un Gradio Space
>>> hf download --repo-type=space Wauplin/my-cool-training-space --include="*.py" --include="requirements.txt" --local-dir=./spaces/my-cool-training-space
Fetching 3 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 24125.05it/s]
./spaces/my-cool-training-space
```

### T√©l√©charger une r√©vision sp√©cifique

L'argument ci-dessus t√©l√©charge les derniers fichiers depuis la branche `main`. Pour t√©l√©charger depuis une autre branche ou une r√©vision de r√©f√©rence (par exemple, d'une PR), utilisez l'option `--revision` :

```bash
>>> hf download bigcode/the-stack --repo-type=dataset --revision=v1.1 --include="data/python/*" --local-dir=./datasets/the-stack-python
Fetching 206 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 206/206 [02:31<00:00,  1.36it/s]
./datasets/the-stack-python
```

### Dry-run mode

Si vous souhaitez avoir un aper√ßu des fichiers qui seront t√©l√©charg√©s avant que cela ne se produise r√©ellement, utilisez l'option `--dry-run`. Cela s'av√®re utile lorsque vous souhaitez t√©l√©charger un d√©p√¥t entier avec des patterns `--include` et `--exclude` mais que vous n'√™tes pas s√ªr que le pattern est correct. L'exemple suivant liste tous les fichiers du d√©p√¥t _adept/fuyu-8b_ sans t√©l√©charger quoi que ce soit :

```bash
>>> hf download adept/fuyu-8b --dry-run
config.json                        -
generation_config.json             -
handler.py                         -
model-00001-of-00002.safetensors   4.96G
model-00002-of-00002.safetensors   543.5M
model.safetensors.index.json       -
onnx/config.json                   -
onnx/decoder_model.onnx            653.7M
onnx/decoder_model_merged.onnx     655.2M
onnx/decoder_with_past_model.onnx  653.7M
pytorch_model.bin.index.json       -
pytorch_model-00001-of-00002.bin   5.0G
pytorch_model-00002-of-00002.bin   548.1M
requirements.txt                   -
special_tokens_map.json            -
tokenizer.json                     -
tokenizer.model                    -
tokenizer_config.json              -
```

Pour plus de d√©tails, consultez le [guide de t√©l√©chargement](./download.md#dry-run-mode).

### Sp√©cifier le r√©pertoire de cache

Si vous n'utilisez pas `--local-dir`, tous les fichiers seront t√©l√©charg√©s par d√©faut dans le r√©pertoire de cache d√©fini par la variable d'environnement `HF_HOME` [environment variable](../package_reference/environment_variables#hfhome). Vous pouvez sp√©cifier un cache personnalis√© en utilisant `--cache-dir` :

```bash
>>> hf download adept/fuyu-8b --cache-dir ./path/to/cache
...
./path/to/cache/models--adept--fuyu-8b/snapshots/ddcacbcf5fdf9cc59ff01f6be6d6662624d9c745
```

### Sp√©cifier un jeton

Pour acc√©der aux d√©p√¥ts priv√©s ou √† acc√®s restreint, vous devez utiliser un jeton. Par d√©faut, le cli utilise le jeton enregistr√© localement (en utilisant `hf auth login`). Si vous souhaitez vous authentifier explicitement, utilisez l'option `--token` :

```bash
>>> hf download gpt2 config.json --token=hf_****
/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
```

### Mode silencieux

Par d√©faut, la commande `hf download` sera verbeuse. Elle affichera des d√©tails tels que des messages d'avertissement, des informations sur les fichiers t√©l√©charg√©s et des barres de progression. Si vous souhaitez masquer tout cela, utilisez l'option `--quiet`. Seule la derni√®re ligne (c'est-√†-dire le chemin vers les fichiers t√©l√©charg√©s) est affich√©e. Cela peut s'av√©rer utile si vous souhaitez passer la sortie √† une autre commande dans un script.

```bash
>>> hf download gpt2 --quiet
/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10
```

### Timeout de t√©l√©chargement

Sur les machines avec des connexions lentes, vous pourriez rencontrer des probl√®mes de timeout comme celui-ci :
```bash
`httpx.TimeoutException: (TimeoutException("HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a33d910c-84c6-4514-8362-c705e2039d38)')`
```

Pour att√©nuer ce probl√®me, vous pouvez d√©finir la variable d'environnement `HF_HUB_DOWNLOAD_TIMEOUT` avec une valeur plus √©lev√©e (la valeur par d√©faut est 10) :
```bash
export HF_HUB_DOWNLOAD_TIMEOUT=30
```

Pour plus de d√©tails, consultez la [r√©f√©rence des variables d'environnement](../package_reference/environment_variables#hfhubdownloadtimeout). Et relancez votre commande de t√©l√©chargement.

## hf upload

Utilisez la commande `hf upload` pour uploader des fichiers vers le Hub directement. En interne, elle utilise les m√™mes helpers [`upload_file`] et [`upload_folder`] d√©crits dans le guide [Upload](./upload). Dans les exemples ci-dessous, nous passerons en revue les cas d'utilisation les plus courants. Pour une liste compl√®te des options disponibles, vous pouvez ex√©cuter :

```bash
>>> hf upload --help
```

### Uploader un dossier entier

L'utilisation par d√©faut pour cette commande est :

```bash
# Usage:  hf upload [repo_id] [local_path] [path_in_repo]
```

Pour uploader le r√©pertoire actuel √† la racine du d√©p√¥t, utilisez :

```bash
>>> hf upload my-cool-model . .
https://huggingface.co/Wauplin/my-cool-model/tree/main/
```

> [!TIP]
> Si le d√©p√¥t n'existe pas encore, il sera cr√©√© automatiquement.

Vous pouvez √©galement uploader un dossier sp√©cifique :

```bash
>>> hf upload my-cool-model ./models .
https://huggingface.co/Wauplin/my-cool-model/tree/main/
```

Enfin, vous pouvez uploader un dossier vers une destination sp√©cifique sur le d√©p√¥t :

```bash
>>> hf upload my-cool-model ./path/to/curated/data /data/train
https://huggingface.co/Wauplin/my-cool-model/tree/main/data/train
```

### Uploader un fichier unique

Vous pouvez √©galement uploader un fichier unique en configurant `local_path` pour pointer vers un fichier sur votre machine. Si c'est le cas, `path_in_repo` est optionnel et sera par d√©faut le nom de votre fichier local :

```bash
>>> hf upload Wauplin/my-cool-model ./models/model.safetensors
https://huggingface.co/Wauplin/my-cool-model/blob/main/model.safetensors
```

Si vous souhaitez uploader un fichier unique vers un r√©pertoire sp√©cifique, configurez `path_in_repo` en cons√©quence :

```bash
>>> hf upload Wauplin/my-cool-model ./models/model.safetensors /vae/model.safetensors
https://huggingface.co/Wauplin/my-cool-model/blob/main/vae/model.safetensors
```

### Uploader plusieurs fichiers

Pour uploader plusieurs fichiers depuis un dossier en une seule fois sans uploader le dossier entier, utilisez les patterns `--include` et `--exclude`. Cela peut √©galement √™tre combin√© avec l'option `--delete` pour supprimer des fichiers sur le d√©p√¥t tout en uploadant de nouveaux. Dans l'exemple ci-dessous, nous synchronisons le Space local en supprimant les fichiers distants et en uploadant tous les fichiers sauf ceux du r√©pertoire `/logs` :

```bash
# Synchroniser le Space local avec le Hub (uploader de nouveaux fichiers sauf depuis logs/, supprimer les fichiers retir√©s)
>>> hf upload Wauplin/space-example --repo-type=space --exclude="/logs/*" --delete="*" --commit-message="Sync local Space with Hub"
...
```

### Uploader vers un dataset ou un Space

Pour uploader vers un dataset ou un Space, utilisez l'option `--repo-type` :

```bash
>>> hf upload Wauplin/my-cool-dataset ./data /train --repo-type=dataset
...
```

### Uploader vers une organisation

Pour uploader du contenu vers un d√©p√¥t appartenant √† une organisation plut√¥t qu'un d√©p√¥t personnel, vous devez le sp√©cifier explicitement dans le `repo_id` :

```bash
>>> hf upload MyCoolOrganization/my-cool-model . .
https://huggingface.co/MyCoolOrganization/my-cool-model/tree/main/
```

### Uploader vers une r√©vision sp√©cifique

Par d√©faut, les fichiers sont upload√©s vers la branche `main`. Si vous souhaitez uploader des fichiers vers une autre branche ou r√©f√©rence, utilisez l'option `--revision` :

```bash
# Uploader des fichiers vers une PR
>>> hf upload bigcode/the-stack . . --repo-type dataset --revision refs/pr/104
...
```

**Note :** si `revision` n'existe pas et que `--create-pr` n'est pas d√©fini, une branche sera cr√©√©e automatiquement depuis la branche `main`.

### Uploader et cr√©er une PR

Si vous n'avez pas la permission de pousser vers un d√©p√¥t, vous devez ouvrir une PR et informer les auteurs des modifications que vous souhaitez apporter. Cela peut √™tre fait en configurant l'option `--create-pr` :

```bash
# Cr√©er une PR et uploader les fichiers vers celle-ci
>>> hf upload bigcode/the-stack . . --repo-type dataset --revision refs/pr/104
https://huggingface.co/datasets/bigcode/the-stack/blob/refs%2Fpr%2F104/
```

### Uploader √† intervalles r√©guliers

Dans certains cas, vous pourriez vouloir pousser des mises √† jour r√©guli√®res vers un d√©p√¥t. Par exemple, cela est utile si vous entra√Ænez un mod√®le et que vous souhaitez uploader le dossier de logs toutes les 10 minutes. Vous pouvez le faire en utilisant l'option `--every` :

```bash
# Uploader de nouveaux logs toutes les 10 minutes
hf upload training-model logs/ --every=10
```

### Sp√©cifier un message de commit

Utilisez `--commit-message` et `--commit-description` pour d√©finir un message et une description personnalis√©s pour votre commit au lieu de ceux par d√©faut

```bash
>>> hf upload Wauplin/my-cool-model ./models . --commit-message="Epoch 34/50" --commit-description="Val accuracy: 68%. Check tensorboard for more details."
...
https://huggingface.co/Wauplin/my-cool-model/tree/main
```

### Sp√©cifier un jeton

Pour uploader des fichiers, vous devez utiliser un jeton. Par d√©faut, le jeton enregistr√© localement (en utilisant `hf auth login`) sera utilis√©. Si vous souhaitez vous authentifier explicitement, utilisez l'option `--token` :

```bash
>>> hf upload Wauplin/my-cool-model ./models . --token=hf_****
...
https://huggingface.co/Wauplin/my-cool-model/tree/main
```

### Mode silencieux

Par d√©faut, la commande `hf upload` sera verbeuse. Elle affichera des d√©tails tels que des messages d'avertissement, des informations sur les fichiers upload√©s et des barres de progression. Si vous souhaitez masquer tout cela, utilisez l'option `--quiet`. Seule la derni√®re ligne (c'est-√†-dire l'URL vers les fichiers upload√©s) est affich√©e. Cela peut s'av√©rer utile si vous souhaitez passer la sortie √† une autre commande dans un script.

```bash
>>> hf upload Wauplin/my-cool-model ./models . --quiet
https://huggingface.co/Wauplin/my-cool-model/tree/main
```

## hf repo

`hf repo` vous permet de cr√©er, supprimer, d√©placer des d√©p√¥ts et mettre √† jour leurs param√®tres sur le Hugging Face Hub. Elle inclut √©galement des sous-commandes pour g√©rer les branches et les tags.

### Cr√©er un d√©p√¥t

```bash
>>> hf repo create Wauplin/my-cool-model
Successfully created Wauplin/my-cool-model on the Hub.
Your repo is now available at https://huggingface.co/Wauplin/my-cool-model
```

Cr√©er un dataset priv√© ou un Space :

```bash
>>> hf repo create my-cool-dataset --repo-type dataset --private
>>> hf repo create my-gradio-space --repo-type space --space-sdk gradio
```

Utilisez `--exist-ok` si le d√©p√¥t peut d√©j√† exister, et `--resource-group-id` pour cibler un groupe de ressources Enterprise.

### Supprimer un d√©p√¥t

```bash
>>> hf repo delete Wauplin/my-cool-model
```

Datasets et Spaces :

```bash
>>> hf repo delete my-cool-dataset --repo-type dataset
>>> hf repo delete my-gradio-space --repo-type space
```

### D√©placer un d√©p√¥t

```bash
>>> hf repo move old-namespace/my-model new-namespace/my-model
```

### Mettre √† jour les param√®tres du d√©p√¥t

```bash
>>> hf repo settings Wauplin/my-cool-model --gated auto
>>> hf repo settings Wauplin/my-cool-model --private true
>>> hf repo settings Wauplin/my-cool-model --private false
```

- `--gated` : l'un de `auto`, `manual`, `false`
- `--private true|false` : d√©finir la confidentialit√© du d√©p√¥t

### G√©rer les branches

```bash
>>> hf repo branch create Wauplin/my-cool-model dev
>>> hf repo branch create Wauplin/my-cool-model release-1 --revision refs/pr/104
>>> hf repo branch delete Wauplin/my-cool-model dev
```

> [!TIP]
> Toutes les commandes acceptent `--repo-type` (l'un de `model`, `dataset`, `space`) et `--token` si vous devez vous authentifier explicitement. Utilisez `--help` sur n'importe quelle commande pour voir toutes les options.


## hf repo-files

Si vous souhaitez supprimer des fichiers d'un d√©p√¥t Hugging Face, utilisez la commande `hf repo-files`.

### Supprimer des fichiers

La sous-commande `hf repo-files delete <repo_id>` vous permet de supprimer des fichiers d'un d√©p√¥t. Voici quelques exemples d'utilisation.

Supprimer un dossier :
```bash
>>> hf repo-files delete Wauplin/my-cool-model folder/
Files correctly deleted from repo. Commit: https://huggingface.co/Wauplin/my-cool-mo...
```

Supprimer plusieurs fichiers :
```bash
>>> hf repo-files delete Wauplin/my-cool-model file.txt folder/pytorch_model.bin
Files correctly deleted from repo. Commit: https://huggingface.co/Wauplin/my-cool-mo...
```

Utiliser des wildcards de style Unix pour supprimer des ensembles de fichiers :
```bash
>>> hf repo-files delete Wauplin/my-cool-model "*.txt" "folder/*.bin"
Files correctly deleted from repo. Commit: https://huggingface.co/Wauplin/my-cool-mo...
```

### Sp√©cifier un jeton

Pour supprimer des fichiers d'un d√©p√¥t, vous devez √™tre authentifi√© et autoris√©. Par d√©faut, le jeton enregistr√© localement (en utilisant `hf auth login`) sera utilis√©. Si vous souhaitez vous authentifier explicitement, utilisez l'option `--token` :

```bash
>>> hf repo-files delete --token=hf_**** Wauplin/my-cool-model file.txt
```

## hf cache ls

Utilisez `hf cache ls` pour inspecter ce qui est stock√© localement dans votre cache Hugging Face. Par d√©faut, elle agr√®ge les informations par d√©p√¥t :

```bash
>>> hf cache ls
ID                          SIZE     LAST_ACCESSED LAST_MODIFIED REFS        
--------------------------- -------- ------------- ------------- ----------- 
dataset/nyu-mll/glue          157.4M 2 days ago    2 days ago    main script 
model/LiquidAI/LFM2-VL-1.6B     3.2G 4 days ago    4 days ago    main        
model/microsoft/UserLM-8b      32.1G 4 days ago    4 days ago    main  

Found 3 repo(s) for a total of 5 revision(s) and 35.5G on disk.
```

Ajoutez `--revisions` pour descendre jusqu'aux snapshots sp√©cifiques, et encha√Ænez les filtres pour vous concentrer sur ce qui compte :

```bash
>>> hf cache ls --filter "size>30g" --revisions
ID                        REVISION                                 SIZE     LAST_MODIFIED REFS 
------------------------- ---------------------------------------- -------- ------------- ---- 
model/microsoft/UserLM-8b be8f2069189bdf443e554c24e488ff3ff6952691    32.1G 4 days ago    main 

Found 1 repo(s) for a total of 1 revision(s) and 32.1G on disk.
```

La commande prend en charge plusieurs formats de sortie pour les scripts : `--format json` affiche des objets structur√©s, `--format csv` √©crit des lignes s√©par√©es par des virgules, et `--quiet` affiche uniquement les ID. Utilisez `--sort` pour ordonner les entr√©es par `accessed`, `modified`, `name`, ou `size` (ajoutez `:asc` ou `:desc` pour contr√¥ler l'ordre), et `--limit` pour restreindre les r√©sultats aux N premi√®res entr√©es. Combinez-les avec `--cache-dir` pour cibler des emplacements de cache alternatifs. Consultez le guide [G√©rer votre cache](./manage-cache) pour des workflows avanc√©s.

Supprimez les entr√©es de cache s√©lectionn√©es avec `hf cache ls --q` en pipant les ID dans `hf cache rm` :

```bash
>>> hf cache rm $(hf cache ls --filter "accessed>1y" -q) -y
About to delete 2 repo(s) totalling 5.31G.
  - model/meta-llama/Llama-3.2-1B-Instruct (entire repo)
  - model/hexgrad/Kokoro-82M (entire repo)
Delete repo: ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct
Delete repo: ~/.cache/huggingface/hub/models--hexgrad--Kokoro-82M
Cache deletion done. Saved 5.31G.
Deleted 2 repo(s) and 2 revision(s); freed 5.31G.
```

## hf cache rm

`hf cache rm` supprime les d√©p√¥ts en cache ou les r√©visions individuelles. Passez un ou plusieurs ID de d√©p√¥t (`model/bert-base-uncased`) ou hashes de r√©vision :

```bash
>>> hf cache rm model/LiquidAI/LFM2-VL-1.6B
About to delete 1 repo(s) totalling 3.2G.
  - model/LiquidAI/LFM2-VL-1.6B (entire repo)
Proceed with deletion? [y/N]: y
Delete repo: ~/.cache/huggingface/hub/models--LiquidAI--LFM2-VL-1.6B
Cache deletion done. Saved 3.2G.
Deleted 1 repo(s) and 2 revision(s); freed 3.2G.
```

M√©langez des d√©p√¥ts et des r√©visions sp√©cifiques dans le m√™me appel. Utilisez `--dry-run` pour pr√©visualiser l'impact, ou `--yes` pour ignorer le message de confirmation dans les scripts automatis√©s :

```bash
>>> hf cache rm model/t5-small 8f3ad1c --dry-run
About to delete 1 repo(s) and 1 revision(s) totalling 1.1G.
  - model/t5-small:
      8f3ad1c [main] 1.1G
Dry run: no files were deleted.
```

Lors de travaux en dehors de l'emplacement de cache par d√©faut, associez la commande avec `--cache-dir PATH`.

## hf cache prune

`hf cache prune` est un raccourci qui supprime toutes les r√©visions d√©tach√©es (non r√©f√©renc√©es) dans votre cache. Cela ne conserve que les r√©visions qui sont toujours accessibles via une branche ou un tag :

```bash
>>> hf cache prune
About to delete 3 unreferenced revision(s) (2.4G total).
  - model/t5-small:
      1c610f6b [refs/pr/1] 820.1M
      d4ec9b72 [(detached)] 640.5M
  - dataset/google/fleurs:
      2b91c8dd [(detached)] 937.6M
Proceed? [y/N]: y
Deleted 3 unreferenced revision(s); freed 2.4G.
```

Comme avec les autres commandes de cache, `--dry-run`, `--yes`, et `--cache-dir` sont disponibles. R√©f√©rez-vous au guide [G√©rer votre cache](./manage-cache) pour plus d'exemples.

## hf cache verify

Utilisez `hf cache verify` pour valider les fichiers locaux par rapport √† leurs checksums sur le Hub. Vous pouvez v√©rifier soit un snapshot du cache soit un r√©pertoire local normal.

Exemples :

```bash
# V√©rifier la r√©vision main d'un mod√®le dans le cache
>>> hf cache verify deepseek-ai/DeepSeek-OCR

# V√©rifier une r√©vision sp√©cifique
>>> hf cache verify deepseek-ai/DeepSeek-OCR --revision refs/pr/5
>>> hf cache verify deepseek-ai/DeepSeek-OCR --revision ef93bf4a377c5d5ed9dca78e0bc4ea50b26fe6a4

# V√©rifier un d√©p√¥t priv√©
>>> hf cache verify me/private-model --token hf_***

# V√©rifier un dataset
>>> hf cache verify karpathy/fineweb-edu-100b-shuffle --repo-type dataset

# V√©rifier les fichiers dans un r√©pertoire local
>>> hf cache verify deepseek-ai/DeepSeek-OCR --local-dir /path/to/repo
```

Par d√©faut, la commande avertit sur les fichiers manquants ou suppl√©mentaires. Utilisez des drapeaux pour transformer ces avertissements en erreurs :

```bash
>>> hf cache verify deepseek-ai/DeepSeek-OCR --fail-on-missing-files --fail-on-extra-files
```

En cas de succ√®s, vous verrez un r√©sum√© :

```text
‚úÖ Verified 13 file(s) for 'deepseek-ai/DeepSeek-OCR' (model) in ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6
  All checksums match.
```

Si des non-correspondances sont d√©tect√©es, la commande affiche une liste d√©taill√©e et se termine avec un statut non nul.

## hf repo tag create

La commande `hf repo tag create` vous permet de tagger, untagger et lister les tags pour les d√©p√¥ts.

### Tagger un mod√®le

Pour tagger un d√©p√¥t, vous devez fournir le `repo_id` et le nom du `tag` :

```bash
>>> hf repo tag create Wauplin/my-cool-model v1.0
You are about to create tag v1.0 on model Wauplin/my-cool-model
Tag v1.0 created on Wauplin/my-cool-model
```

### Tagger un mod√®le √† une r√©vision sp√©cifique

Si vous souhaitez tagger une r√©vision sp√©cifique, vous pouvez utiliser l'option `--revision`. Par d√©faut, le tag sera cr√©√© sur la branche `main` :

```bash
>>> hf repo tag create Wauplin/my-cool-model v1.0 --revision refs/pr/104
You are about to create tag v1.0 on model Wauplin/my-cool-model
Tag v1.0 created on Wauplin/my-cool-model
```

### Tagger un dataset ou un Space

Si vous souhaitez tagger un dataset ou Space, vous devez sp√©cifier l'option `--repo-type` :

```bash
>>> hf repo tag create bigcode/the-stack v1.0 --repo-type dataset
You are about to create tag v1.0 on dataset bigcode/the-stack
Tag v1.0 created on bigcode/the-stack
```

### Lister les tags

Pour lister tous les tags d'un d√©p√¥t, utilisez l'option `-l` ou `--list` :

```bash
>>> hf repo tag create Wauplin/gradio-space-ci -l --repo-type space
Tags for space Wauplin/gradio-space-ci:
0.2.2
0.2.1
0.2.0
0.1.2
0.0.2
0.0.1
```

### Supprimer un tag

Pour supprimer un tag, utilisez l'option `-d` ou `--delete` :

```bash
>>> hf repo tag create -d Wauplin/my-cool-model v1.0
You are about to delete tag v1.0 on model Wauplin/my-cool-model
Proceed? [Y/n] y
Tag v1.0 deleted on Wauplin/my-cool-model
```

Vous pouvez √©galement passer `-y` pour ignorer l'√©tape de confirmation.

## hf env

La commande `hf env` affiche des d√©tails sur la configuration de votre machine. Ceci est utile lorsque vous ouvrez un probl√®me sur [GitHub](https://github.com/huggingface/huggingface_hub) pour aider les mainteneurs √† enqu√™ter sur votre probl√®me.

```bash
>>> hf env

Copy-and-paste the text below in your GitHub issue.

- huggingface_hub version: 1.0.0.rc6
- Platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.35
- Python version: 3.11.14
- Running in iPython ?: No
- Running in notebook ?: No
- Running in Google Colab ?: No
- Running in Google Colab Enterprise ?: No
- Token path ?: /home/wauplin/.cache/huggingface/token
- Has saved token ?: True
- Who am I ?: Wauplin
- Configured git credential helpers: store
- Installation method: unknown
- Torch: N/A
- httpx: 0.28.1
- hf_xet: 1.1.10
- gradio: 5.41.1
- tensorboard: N/A
- pydantic: 2.11.7
- ENDPOINT: https://huggingface.co
- HF_HUB_CACHE: /home/wauplin/.cache/huggingface/hub
- HF_ASSETS_CACHE: /home/wauplin/.cache/huggingface/assets
- HF_TOKEN_PATH: /home/wauplin/.cache/huggingface/token
- HF_STORED_TOKENS_PATH: /home/wauplin/.cache/huggingface/stored_tokens
- HF_HUB_OFFLINE: False
- HF_HUB_DISABLE_TELEMETRY: False
- HF_HUB_DISABLE_PROGRESS_BARS: None
- HF_HUB_DISABLE_SYMLINKS_WARNING: False
- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False
- HF_HUB_DISABLE_IMPLICIT_TOKEN: False
- HF_HUB_DISABLE_XET: False
- HF_HUB_ETAG_TIMEOUT: 10
- HF_HUB_DOWNLOAD_TIMEOUT: 10
```

## hf jobs

Ex√©cutez des jobs de calcul sur l'infrastructure Hugging Face avec une interface famili√®re de type Docker.

`hf jobs` est un outil en ligne de commande qui vous permet d'ex√©cuter n'importe quoi sur l'infrastructure de Hugging Face (y compris les GPU et TPU !) avec des commandes simples. Pensez √† `docker run`, mais pour ex√©cuter du code sur des A100.

```bash
# Ex√©cuter directement du code Python
>>> hf jobs run python:3.12 python -c 'print("Hello from the cloud!")'

# Utiliser des GPU sans aucune configuration
>>> hf jobs run --flavor a10g-small pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
... python -c "import torch; print(torch.cuda.get_device_name())"

# Ex√©cuter dans un compte d'organisation
>>> hf jobs run --namespace my-org-name python:3.12 python -c "print('Running in an org account')"

# Ex√©cuter depuis des Hugging Face Spaces
>>> hf jobs run hf.co/spaces/lhoestq/duckdb duckdb -c "select 'hello world'"

# Ex√©cuter un script Python avec `uv` (exp√©rimental)
>>> hf jobs uv run my_script.py
```

### ‚ú® Fonctionnalit√©s cl√©s

- üê≥ **CLI de type Docker** : Commandes famili√®res (`run`, `ps`, `logs`, `inspect`) pour ex√©cuter et g√©rer les jobs
- üî• **N'importe quel mat√©riel** : Des CPU aux GPU A100 et pods TPU - changez avec un simple drapeau
- üì¶ **Ex√©cutez n'importe quoi** : Utilisez des images Docker, des HF Spaces, ou vos conteneurs personnalis√©s
- üîê **Authentification simple** : Il suffit d'utiliser votre jeton HF
- üìä **Surveillance en direct** : Streamer les logs en temps r√©el, comme si vous ex√©cutiez localement
- üí∞ **Paiement √† l'utilisation** : Ne payez que pour les secondes que vous utilisez

> [!TIP]
> Les **Hugging Face Jobs** ne sont disponibles que pour les [utilisateurs Pro](https://huggingface.co/pro) et les [organisations Team ou Enterprise](https://huggingface.co/enterprise). Mettez √† niveau votre abonnement pour commencer !

### D√©marrage rapide

#### 1. Ex√©cuter votre premier job

```bash
# Ex√©cuter un simple script Python
>>> hf jobs run python:3.12 python -c "print('Hello from HF compute!')"
```

Cette commande ex√©cute le job et affiche les logs. Vous pouvez passer `--detach` pour ex√©cuter le Job en arri√®re-plan et n'afficher que l'ID du Job.

#### 2. V√©rifier le statut du job

```bash
# Lister vos jobs en cours d'ex√©cution
>>> hf jobs ps

# Inspecter le statut d'un job
>>> hf jobs inspect <job_id>

# Afficher les logs d'un job
>>> hf jobs logs <job_id>

# Annuler un job
>>> hf jobs cancel <job_id>
```

#### 3. Ex√©cuter sur GPU

Vous pouvez √©galement ex√©cuter des jobs sur des GPU ou TPU avec l'option `--flavor`. Par exemple, pour ex√©cuter un job PyTorch sur un GPU A10G :

```bash
# Utiliser un GPU A10G pour v√©rifier PyTorch CUDA
>>> hf jobs run --flavor a10g-small pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
... python -c "import torch; print(f"This code ran with the following GPU: {torch.cuda.get_device_name()}")"
```

L'ex√©cution de ceci affichera la sortie suivante !

```bash
This code ran with the following GPU: NVIDIA A10G
```

Vous ex√©cutez maintenant du code sur l'infrastructure de Hugging Face.

### Cas d'utilisation courants

- **Entra√Ænement de mod√®les** : Affinez ou entra√Ænez des mod√®les sur des GPU (T4, A10G, A100) sans g√©rer d'infrastructure
- **G√©n√©ration de donn√©es synth√©tiques** : G√©n√©rez des datasets √† grande √©chelle en utilisant des LLM sur du mat√©riel puissant
- **Traitement de donn√©es** : Traitez des datasets massifs avec des configurations haute-CPU pour des charges de travail parall√®les
- **Inf√©rence par lots** : Ex√©cutez des inf√©rences hors ligne sur des milliers d'√©chantillons en utilisant des configurations GPU optimis√©es
- **Exp√©riences & Benchmarks** : Ex√©cutez des exp√©riences ML sur du mat√©riel coh√©rent pour des r√©sultats reproductibles
- **D√©veloppement & D√©bogage** : Testez du code GPU sans configuration CUDA locale

### Passer des variables d'environnement et des secrets

Vous pouvez passer des variables d'environnement √† votre job en utilisant 

```bash
# Passer des variables d'environnement
>>> hf jobs run -e FOO=foo -e BAR=bar python:3.12 python -c "import os; print(os.environ['FOO'], os.environ['BAR'])"
```

```bash
# Passer un environnement depuis un fichier .env local
>>> hf jobs run --env-file .env python:3.12 python -c "import os; print(os.environ['FOO'], os.environ['BAR'])"
```

```bash
# Passer des secrets - ils seront chiffr√©s c√¥t√© serveur
>>> hf jobs run -s MY_SECRET=psswrd python:3.12 python -c "import os; print(os.environ['MY_SECRET'])"
```

```bash
# Passer des secrets depuis un fichier .env.secrets local - ils seront chiffr√©s c√¥t√© serveur
>>> hf jobs run --secrets-file .env.secrets python:3.12 python -c "import os; print(os.environ['MY_SECRET'])"
```

> [!TIP]
> Utilisez `--secrets HF_TOKEN` pour passer votre jeton Hugging Face local implicitement.
> Avec cette syntaxe, le secret est r√©cup√©r√© depuis la variable d'environnement.
> Pour `HF_TOKEN`, il peut lire le fichier de jeton situ√© dans le dossier home de Hugging Face si la variable d'environnement n'est pas d√©finie.

### Mat√©riel

Options `--flavor` disponibles :

- CPU : `cpu-basic`, `cpu-upgrade`
- GPU : `t4-small`, `t4-medium`, `l4x1`, `l4x4`, `a10g-small`, `a10g-large`, `a10g-largex2`, `a10g-largex4`,`a100-large`
- TPU : `v5e-1x1`, `v5e-2x2`, `v5e-2x4`

(mis √† jour en 07/2025 depuis la [documentation suggested_hardware](https://huggingface.co/docs/hub/en/spaces-config-reference) de Hugging Face)

### Scripts UV (Exp√©rimental)

Ex√©cutez des scripts UV (scripts Python avec d√©pendances inline) sur l'infrastructure HF :

```bash
# Ex√©cuter un script UV (cr√©e un d√©p√¥t temporaire)
>>> hf jobs uv run my_script.py

# Ex√©cuter avec un d√©p√¥t persistant
>>> hf jobs uv run my_script.py --repo my-uv-scripts

# Ex√©cuter avec GPU
>>> hf jobs uv run ml_training.py --flavor gpu-t4-small

# Passer des arguments au script
>>> hf jobs uv run process.py input.csv output.parquet

# Ajouter des d√©pendances
>>> hf jobs uv run --with transformers --with torch train.py

# Ex√©cuter un script directement depuis une URL
>>> hf jobs uv run https://huggingface.co/datasets/username/scripts/resolve/main/example.py

# Ex√©cuter une commande
>>> hf jobs uv run --with lighteval python -c "import lighteval"
```

Les scripts UV sont des scripts Python qui incluent leurs d√©pendances directement dans le fichier en utilisant une syntaxe de commentaire sp√©ciale. Cela les rend parfaits pour les t√¢ches autonomes qui ne n√©cessitent pas de configurations complexes. En savoir plus sur les scripts UV dans la [documentation UV](https://docs.astral.sh/uv/guides/scripts/).

### Jobs planifi√©s

Planifiez et g√©rez des jobs qui s'ex√©cuteront sur l'infrastructure HF.

Le planning doit √™tre l'un de `@annually`, `@yearly`, `@monthly`, `@weekly`, `@daily`, `@hourly`, ou une expression CRON (par exemple, `"0 9 * * 1"` pour 9h tous les lundis).

```bash
# Planifier un job qui s'ex√©cute toutes les heures
>>> hf jobs scheduled run @hourly python:3.12 python -c 'print("This runs every hour!")'

# Utiliser la syntaxe CRON
>>> hf jobs scheduled run "*/5 * * * *" python:3.12 python -c 'print("This runs every 5 minutes!")'

# Planifier avec GPU
>>> hf jobs scheduled run @hourly --flavor a10g-small pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
... python -c "import torch; print(f"This code ran with the following GPU: {torch.cuda.get_device_name()}")"

# Planifier un script UV
>>> hf jobs scheduled uv run @hourly my_script.py
```

Utilisez les m√™mes param√®tres que `hf jobs run` pour passer des variables d'environnement, des secrets, un timeout, etc.

G√©rez les jobs planifi√©s en utilisant

```bash
# Lister vos jobs planifi√©s actifs
>>> hf jobs scheduled ps

# Inspecter le statut d'un job
>>> hf jobs scheduled inspect <scheduled_job_id>

# Suspendre (mettre en pause) un job planifi√©
>>> hf jobs scheduled suspend <scheduled_job_id>

# Reprendre un job planifi√©
>>> hf jobs scheduled resume <scheduled_job_id>

# Supprimer un job planifi√©
>>> hf jobs scheduled delete <scheduled_job_id>
```

## hf endpoints

Utilisez `hf endpoints` pour lister, d√©ployer, d√©crire et g√©rer les Inference Endpoints directement depuis le terminal. L'alias h√©rit√©
`hf inference-endpoints` reste disponible pour la compatibilit√©.

```bash
# Lister les endpoints dans votre namespace
>>> hf endpoints ls

# D√©ployer un endpoint depuis le Model Catalog
>>> hf endpoints catalog deploy --repo openai/gpt-oss-120b --name my-endpoint

# D√©ployer un endpoint depuis le Hugging Face Hub 
>>> hf endpoints deploy my-endpoint --repo gpt2 --framework pytorch --accelerator cpu --instance-size x2 --instance-type intel-icl

# Lister les entr√©es du catalogue
>>> hf endpoints catalog ls

# Afficher le statut et les m√©tadonn√©es
>>> hf endpoints describe my-endpoint

# Mettre l'endpoint en pause
>>> hf endpoints pause my-endpoint

# Supprimer sans invite de confirmation
>>> hf endpoints delete my-endpoint --yes
```

> [!TIP]
> Ajoutez `--namespace` pour cibler une organisation, `--token` pour remplacer l'authentification.
