<!--‚ö†Ô∏è Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Dateien auf den Hub hochladen

Das Teilen Ihrer Dateien und Arbeiten ist ein wichtiger Aspekt des Hubs. Das `huggingface_hub` bietet mehrere Optionen, um Ihre Dateien auf den Hub hochzuladen. Sie k√∂nnen diese Funktionen unabh√§ngig verwenden oder sie in Ihre Bibliothek integrieren, um es Ihren Benutzern zu erleichtern, mit dem Hub zu interagieren. In dieser Anleitung erfahren Sie, wie Sie Dateien hochladen:

- ohne Git zu verwenden.
- mit [Git LFS](https://git-lfs.github.com/) wenn die Dateien sehr gro√ü sind.
- mit dem `commit`-Context-Manager.
- mit der Funktion [`~Repository.push_to_hub`].

Wenn Sie Dateien auf den Hub hochladen m√∂chten, m√ºssen Sie sich bei Ihrem Hugging Face-Konto anmelden:

- Melden Sie sich bei Ihrem Hugging Face-Konto mit dem folgenden Befehl an:

  ```bash
  hf auth login
  # oder mit einer Umgebungsvariable
  hf auth login --token $HUGGINGFACE_TOKEN
  ```

- Alternativ k√∂nnen Sie sich in einem Notebook oder einem Skript programmatisch mit [`login`] anmelden:

  ```python
  >>> from huggingface_hub import login
  >>> login()
  ```

  Wenn es in einem Jupyter- oder Colaboratory-Notebook ausgef√ºhrt wird, startet [`login`] ein Widget, √ºber das Sie Ihren Hugging Face-Zugriffstoken eingeben k√∂nnen. Andernfalls wird eine Meldung im Terminal angezeigt.

  Es ist auch m√∂glich, sich programmatisch ohne das Widget anzumelden, indem Sie den Token direkt an [`login`] √ºbergeben. Seien Sie jedoch vorsichtig, wenn Sie Ihr Notebook teilen. Es ist am Besten, den Token aus einem sicheren Passwortspeicher zu laden, anstatt ihn in Ihrem Colaboratory-Notebook zu speichern.

## Datei hochladen

Sobald Sie ein Repository mit [`create_repo`] erstellt haben, k√∂nnen Sie mit [`upload_file`] eine Datei in Ihr Repository hochladen.

Geben Sie den Pfad der hochzuladenden Datei, den Ort, an den Sie die Datei im Repository hochladen m√∂chten, und den Namen des Repositories an, zu dem Sie die Datei hinzuf√ºgen m√∂chten. Abh√§ngig von Ihrem Repository-Typ k√∂nnen Sie optional den Repository-Typ als `dataset`, `model`, oder `space` festlegen.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.upload_file(
...     path_or_fileobj="/path/to/local/folder/README.md",
...     path_in_repo="README.md",
...     repo_id="username/test-dataset",
...     repo_type="dataset",
... )
```

## Ordner hochladen

Verwenden Sie die Funktion [`upload_folder`], um einen lokalen Ordner in ein vorhandenes Repository hochzuladen. Geben Sie den Pfad des lokalen Ordners an, den Sie hochladen m√∂chten, an welchem Ort Sie den Ordner im Repository hochladen m√∂chten, und den Namen des Repositories, zu dem Sie den Ordner hinzuf√ºgen m√∂chten. Abh√§ngig von Ihrem Repository-Typ k√∂nnen Sie optional den Repository-Typ als `dataset`, `model`, oder `space` festlegen.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()

# Den gesamten Inhalt aus dem lokalen Ordner in den entfernten Space hoch laden.
# Standardm√§√üig werden Dateien im Hauptverzeichnis des Repos hochgeladen
>>> api.upload_folder(
...     folder_path="/path/to/local/space",
...     repo_id="username/my-cool-space",
...     repo_type="space",
... )
```

Verwenden Sie die Argumente `allow_patterns` und `ignore_patterns`, um anzugeben, welche Dateien hochgeladen werden sollen. Diese Parameter akzeptieren entweder ein einzelnes Muster oder eine Liste von Mustern. Muster sind Standard-Wildcards (globbing patterns) wie [hier](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm) dokumentiert. Wenn sowohl `allow_patterns` als auch `ignore_patterns` angegeben werden, gelten beide Einschr√§nkungen. Standardm√§√üig werden alle Dateien aus dem Ordner hochgeladen.

Jeder `.git/`-Ordner in einem Unterverzeichnis wird ignoriert. Bitte beachten Sie jedoch, dass die `.gitignore`-Datei nicht ber√ºcksichtigt wird. Dies bedeutet, dass Sie `allow_patterns` und `ignore_patterns` verwenden m√ºssen, um anzugeben, welche Dateien stattdessen hochgeladen werden sollen.

```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder",
...     path_in_repo="my-dataset/train", # Hochladen in einen bestimmten Ordner
...     repo_id="username/test-dataset",
...     repo_type="dataset",
...     ignore_patterns="**/logs/*.txt", # Alle Textprotokolle ignorieren
... )
```

Sie k√∂nnen auch das Argument `delete_patterns` verwenden, um Dateien anzugeben, die Sie im selben Commit aus dem Repo l√∂schen m√∂chten. Dies kann n√ºtzlich sein, wenn Sie einen entfernten Ordner reinigen m√∂chten, bevor Sie Dateien darin ablegen und nicht wissen, welche Dateien bereits vorhanden sind.

Im folgenden Beispiel wird der lokale Ordner `./logs` in den entfernten Ordner `/experiment/logs/` hochgeladen. Es werden nur txt-Dateien hochgeladen, aber davor werden alle vorherigen Protokolle im Repo gel√∂scht. All dies in einem einzigen Commit.

```py
>>> api.upload_folder(
...     folder_path="/path/to/local/folder/logs",
...     repo_id="username/trained-model",
...     path_in_repo="experiment/logs/",
...     allow_patterns="*.txt", # Alle lokalen Textdateien hochladen
...     delete_patterns="*.txt", # Vorher alle enfernten Textdateien l√∂schen
... )
```

## Erweiterte Funktionen

In den meisten F√§llen ben√∂tigen Sie nicht mehr als [`upload_file`] und [`upload_folder`], um Ihre Dateien auf den Hub hochzuladen. Das `huggingface_hub` bietet jedoch fortschrittlichere Funktionen, um die Dinge einfacher zu machen. Schauen wir sie uns an!


### Nicht blockierende Uploads

In einigen F√§llen m√∂chten Sie Daten hochladen, ohne Ihren Hauptthread zu blockieren. Dies ist besonders n√ºtzlich, um Protokolle und Artefakte hochzuladen, w√§hrend Sie weiter trainieren. Um dies zu tun, k√∂nnen Sie das Argument `run_as_future` in beiden [`upload_file`] und [`upload_folder`] verwenden. Dies gibt ein [`concurrent.futures.Future`](https://docs.python.org/3/library/concurrent.futures.html#future-objects)-Objekt zur√ºck, mit dem Sie den Status des Uploads √ºberpr√ºfen k√∂nnen.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> future = api.upload_folder( # Hochladen im Hintergrund (nicht blockierende Aktion)
...     repo_id="username/my-model",
...     folder_path="checkpoints-001",
...     run_as_future=True,
... )
>>> future
Future(...)
>>> future.done()
False
>>> future.result() # Warten bis der Upload abgeschlossen ist (blockierende Aktion)
...
```

> [!TIP]
> Hintergrund-Aufgaben werden in die Warteschlange gestellt, wenn `run_as_future=True` verwendet wird. Das bedeutet, dass garantiert wird, dass die Aufgaben in der richtigen Reihenfolge ausgef√ºhrt werden.

Auch wenn Hintergrundaufgaben haupts√§chlich dazu dienen, Daten hochzuladen/Commits zu erstellen, k√∂nnen Sie jede gew√ºnschte Methode in die Warteschlange stellen, indem Sie [`run_as_future`] verwenden. Sie k√∂nnen es beispielsweise verwenden, um ein Repo zu erstellen und dann Daten im Hintergrund dorthin hochzuladen. Das integrierte Argument `run_as_future` in Upload-Methoden ist lediglich ein Alias daf√ºr.

```py
>>> from huggingface_hub import HfApi
>>> api = HfApi()
>>> api.run_as_future(api.create_repo, "username/my-model", exists_ok=True)
Future(...)
>>> api.upload_file(
...     repo_id="username/my-model",
...     path_in_repo="file.txt",
...     path_or_fileobj=b"file content",
...     run_as_future=True,
... )
Future(...)
```

### Ordner in Teilen hochladen

Mit [`upload_folder`] k√∂nnen Sie ganz einfach einen gesamten Ordner ins Hub hochladen. Bei gro√üen Ordnern (Tausende von Dateien oder Hunderte von GB) kann dies jedoch immer noch herausfordernd sein. Wenn Sie einen Ordner mit vielen Dateien haben, m√∂chten Sie ihn m√∂glicherweise in mehreren Commits hochladen. Wenn w√§hrend des Uploads ein Fehler oder ein Verbindungsproblem auftritt, m√ºssen Sie den Vorgang nicht von Anfang an wiederholen.

Um einen Ordner in mehreren Commits hochzuladen, √ºbergeben Sie einfach `multi_commits=True` als Argument. Intern wird `huggingface_hub` die hochzuladenden/zu l√∂schenden Dateien auflisten und sie in mehrere Commits aufteilen. Die "Strategie" (d.h. wie die Commits aufgeteilt werden) basiert auf der Anzahl und Gr√∂√üe der hochzuladenden Dateien. Ein PR wird im Hub ge√∂ffnet, um alle Commits zu pushen. Sobald der PR bereit ist, werden die Commits zu einem einzigen Commit zusammengefasst. Wenn der Prozess unterbrochen wird, bevor er abgeschlossen ist, k√∂nnen Sie Ihr Skript erneut ausf√ºhren, um den Upload fortzusetzen. Der erstellte PR wird automatisch erkannt und der Upload setzt dort fort, wo er gestoppt wurde. Es wird empfohlen, `multi_commits_verbose=True` zu √ºbergeben, um ein besseres Verst√§ndnis f√ºr den Upload und dessen Fortschritt zu erhalten.

Das untenstehende Beispiel l√§dt den Ordner "checkpoints" in ein Dataset in mehreren Commits hoch. Ein PR wird im Hub erstellt und automatisch zusammengef√ºhrt, sobald der Upload abgeschlossen ist. Wenn Sie m√∂chten, dass der PR offen bleibt und Sie ihn manuell √ºberpr√ºfen k√∂nnen, √ºbergeben Sie `create_pr=True`.

```py
>>> upload_folder(
...     folder_path="local/checkpoints",
...     repo_id="username/my-dataset",
...     repo_type="dataset",
...     multi_commits=True,
...     multi_commits_verbose=True,
... )
```

Wenn Sie die Upload-Strategie besser steuern m√∂chten (d.h. die erstellten Commits), k√∂nnen Sie sich die Low-Level-Methoden [`plan_multi_commits`] und [`create_commits_on_pr`] ansehen.

> [!WARNING]
> `multi_commits` ist noch ein experimentelles Feature. Seine API und sein Verhalten k√∂nnen in Zukunft ohne vorherige Ank√ºndigung ge√§ndert werden.

### Geplante Uploads

Das Hugging Face Hub erleichtert das Speichern und Versionieren von Daten. Es gibt jedoch einige Einschr√§nkungen, wenn Sie dieselbe Datei Tausende von Malen aktualisieren m√∂chten. Sie m√∂chten beispielsweise Protokolle eines Trainingsprozesses oder Benutzerfeedback in einem bereitgestellten Space speichern. In diesen F√§llen macht es Sinn, die Daten als Dataset im Hub hochzuladen, aber es kann schwierig sein, dies richtig zu machen. Der Hauptgrund ist, dass Sie nicht jede Aktualisierung Ihrer Daten versionieren m√∂chten, da dies das git-Repository unbrauchbar machen w√ºrde. Die Klasse [`CommitScheduler`] bietet eine L√∂sung f√ºr dieses Problem.

Die Idee besteht darin, einen Hintergrundjob auszuf√ºhren, der regelm√§√üig einen lokalen Ordner ins Hub schiebt. Nehmen Sie an, Sie haben einen Gradio Space, der als Eingabe einen Text nimmt und zwei √úbersetzungen davon generiert. Der Benutzer kann dann seine bevorzugte √úbersetzung ausw√§hlen. F√ºr jeden Durchlauf m√∂chten Sie die Eingabe, Ausgabe und Benutzerpr√§ferenz speichern, um die Ergebnisse zu analysieren. Dies ist ein perfekter Anwendungsfall f√ºr [`CommitScheduler`]; Sie m√∂chten Daten ins Hub speichern (potenziell Millionen von Benutzerfeedbacks), aber Sie m√ºssen nicht in Echtzeit jede Benutzereingabe speichern. Stattdessen k√∂nnen Sie die Daten lokal in einer JSON-Datei speichern und sie alle 10 Minuten hochladen. Zum Beispiel:

```py
>>> import json
>>> import uuid
>>> from pathlib import Path
>>> import gradio as gr
>>> from huggingface_hub import CommitScheduler

# Definieren Sie die Datei, in der die Daten gespeichert werden sollen. Verwenden Sie UUID, um sicherzustellen, dass vorhandene Daten aus einem fr√ºheren Lauf nicht √ºberschrieben werden.
>>> feedback_file = Path("user_feedback/") / f"data_{uuid.uuid4()}.json"
>>> feedback_folder = feedback_file.parent

# Geplante regelm√§√üige Uploads. Das Remote-Repo und der lokale Ordner werden erstellt, wenn sie noch nicht existieren.
>>> scheduler = CommitScheduler(
...     repo_id="report-translation-feedback",
...     repo_type="dataset",
...     folder_path=feedback_folder,
...     path_in_repo="data",
...     every=10,
... )

# Eine einfache Gradio-Anwendung, die einen Text als Eingabe nimmt und zwei √úbersetzungen generiert. Der Benutzer w√§hlt seine bevorzugte √úbersetzung aus.
>>> def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None:
...     """
...     F√ºge Eingabe/Ausgabe und Benutzerfeedback zu einer JSON-Lines-Datei hinzu und verwende ein Thread-Lock, um gleichzeitiges Schreiben von verschiedenen Benutzern zu vermeiden.
...     """
...     with scheduler.lock:
...         with feedback_file.open("a") as f:
...             f.write(json.dumps({"input": input_text, "output_1": output_1, "output_2": output_2, "user_choice": user_choice}))
...             f.write("\n")

# Starte Gradio
>>> with gr.Blocks() as demo:
>>>     ... # Definiere Gradio Demo + verwende `save_feedback`
>>> demo.launch()
```

Und das war's! Benutzereingabe/-ausgaben und Feedback sind als Dataset auf dem Hub verf√ºgbar. Durch die Verwendung eines eindeutigen JSON-Dateinamens k√∂nnen Sie sicher sein, dass Sie keine Daten von einem vorherigen Lauf oder Daten von anderen Spaces/Replikas √ºberschreiben, die gleichzeitig in dasselbe Repository pushen.

F√ºr weitere Details √ºber den [`CommitScheduler`], hier das Wichtigste:

- **append-only / Nur hinzuf√ºgen:**
      Es wird davon ausgegangen, dass Sie nur Inhalte zum Ordner hinzuf√ºgen. Sie d√ºrfen nur Daten zu bestehenden Dateien hinzuf√ºgen oder neue Dateien erstellen. Das L√∂schen oder √úberschreiben einer Datei k√∂nnte Ihr Repository besch√§digen.
- **git history / git Historie**:
      Der Scheduler wird den Ordner alle every Minuten committen. Um das Git-Repository nicht zu √ºberladen, wird empfohlen, einen minimalen Wert von 5 Minuten festzulegen. Au√üerdem ist der Scheduler darauf ausgelegt, leere Commits zu vermeiden. Wenn im Ordner kein neuer Inhalt erkannt wird, wird der geplante Commit verworfen.
- **errors / Fehler**:
      Der Scheduler l√§uft als Hintergrund-Thread. Er wird gestartet, wenn Sie die Klasse instanziieren, und stoppt nie. Insbesondere, wenn w√§hrend des Uploads ein Fehler auftritt (z. B. Verbindungsproblem), wird der Scheduler ihn stillschweigend ignorieren und beim n√§chsten geplanten Commit erneut versuchen.
- **thread-safety / Thread-Sicherheit**:
      In den meisten F√§llen k√∂nnen Sie davon ausgehen, dass Sie eine Datei schreiben k√∂nnen, ohne sich um eine Lock-Datei k√ºmmern zu m√ºssen. Der Scheduler wird nicht abst√ºrzen oder besch√§digt werden, wenn Sie Inhalte in den Ordner schreiben, w√§hrend er hochl√§dt. In der Praxis ist es m√∂glich, dass bei stark ausgelasteten Apps Probleme mit der Parallelit√§t auftreten. In diesem Fall empfehlen wir, das `scheduler.lock` Lock zu verwenden, um die Thread-Sicherheit zu gew√§hrleisten. Das Lock wird nur gesperrt, wenn der Scheduler den Ordner auf √Ñnderungen √ºberpr√ºft, nicht beim Hochladen von Daten. Sie k√∂nnen sicher davon ausgehen, dass dies das Benutzererlebnis in Ihrem Space nicht beeinflusst.

#### Space Persistenz-Demo

Das Speichern von Daten aus einem Space in einem Dataset auf dem Hub ist der Hauptanwendungsfall f√ºr den [`CommitScheduler`]. Je nach Anwendungsfall m√∂chten Sie Ihre Daten m√∂glicherweise anders strukturieren. Die Struktur muss robust gegen√ºber gleichzeitigen Benutzern und Neustarts sein, was oft das Generieren von UUIDs impliziert. Neben der Robustheit sollten Sie Daten in einem Format hochladen, das von der ü§ó Datasets-Bibliothek f√ºr die sp√§tere Wiederverwendung gelesen werden kann. Wir haben einen [Space](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver) erstellt, der zeigt, wie man verschiedene Datenformate speichert (dies muss m√∂glicherweise f√ºr Ihre speziellen Bed√ºrfnisse angepasst werden).

#### Benutzerdefinierte Uploads

[`CommitScheduler`] geht davon aus, dass Ihre Daten nur hinzugef√ºgt werden und "wie sie sind" hochgeladen werden sollten. Sie m√∂chten jedoch m√∂glicherweise anpassen, wie Daten hochgeladen werden. Dies k√∂nnen Sie tun, indem Sie eine Klasse erstellen, die vom [`CommitScheduler`] erbt und die Methode `push_to_hub` √ºberschreibt (f√ºhlen Sie sich frei, sie nach Belieben zu √ºberschreiben). Es ist garantiert, dass sie alle `every` Minuten in einem Hintergrund-Thread aufgerufen wird. Sie m√ºssen sich keine Gedanken √ºber Parallelit√§t und Fehler machen, aber Sie m√ºssen vorsichtig sein bei anderen Aspekten, wie z. B. dem Pushen von leeren Commits oder doppelten Daten.

Im folgenden (vereinfachten) Beispiel √ºberschreiben wir `push_to_hub`, um alle PNG-Dateien in einem einzigen Archiv zu zippen, um das Repo auf dem Hub nicht zu √ºberladen:

```py
class ZipScheduler(CommitScheduler):
    def push_to_hub(self):
        # 1. Liste PNG-Dateien auf
          png_files = list(self.folder_path.glob("*.png"))
          if len(png_files) == 0:
              return None  # kehre fr√ºh zur√ºck, wenn nichts zu committen ist

        # 2. Zippe PNG-Dateien in ein einzelnes Archiv
        with tempfile.TemporaryDirectory() as tmpdir:
            archive_path = Path(tmpdir) / "train.zip"
            with zipfile.ZipFile(archive_path, "w", zipfile.ZIP_DEFLATED) as zip:
                for png_file in png_files:
                    zip.write(filename=png_file, arcname=png_file.name)

            # 3. Lade das Archiv hoch
            self.api.upload_file(..., path_or_fileobj=archive_path)

        # 4. L√∂sche lokale PNG-Dateien, um sp√§teres erneutes Hochladen zu vermeiden
        for png_file in png_files:
            png_file.unlink()
```

Wenn Sie `push_to_hub` √ºberschreiben, haben Sie Zugriff auf die Attribute vom [`CommitScheduler`] und insbesondere:
- [`HfApi`] Client: `api`
- Ordnerparameter: `folder_path` und `path_in_repo`
- Repo-Parameter: `repo_id`, `repo_type`, `revision`
- Das Thread-Lock: `lock`

> [!TIP]
> F√ºr weitere Beispiele von benutzerdefinierten Schedulern, schauen Sie sich unseren [Demo Space](https://huggingface.co/spaces/Wauplin/space_to_dataset_saver) an, der verschiedene Implementierungen je nach Ihren Anforderungen enth√§lt.

### create_commit

Die Funktionen [`upload_file`] und [`upload_folder`] sind High-Level-APIs, die im Allgemeinen bequem zu verwenden sind. Wir empfehlen, diese Funktionen zuerst auszuprobieren, wenn Sie nicht auf einer niedrigeren Ebene arbeiten m√ºssen. Wenn Sie jedoch auf Commit-Ebene arbeiten m√∂chten, k√∂nnen Sie die Funktion [`create_commit`] direkt verwenden.

Es gibt drei von [`create_commit`] unterst√ºtzte Operationstypen:

- [`CommitOperationAdd`] l√§dt eine Datei in den Hub hoch. Wenn die Datei bereits existiert, werden die Dateiinhalte √ºberschrieben. Diese Operation akzeptiert zwei Argumente:

  - `path_in_repo`: der Repository-Pfad, um eine Datei hochzuladen.
  - `path_or_fileobj`: entweder ein Pfad zu einer Datei auf Ihrem Dateisystem oder ein Datei-√§hnliches Objekt. Dies ist der Inhalt der Datei, die auf den Hub hochgeladen werden soll.
- [`CommitOperationDelete`] entfernt eine Datei oder einen Ordner aus einem Repository. Diese Operation akzeptiert path_in_repo als Argument.
- [`CommitOperationCopy`] kopiert eine Datei innerhalb eines Repositorys. Diese Operation akzeptiert drei Argumente:
  - `src_path_in_repo`: der Repository-Pfad der zu kopierenden Datei.
  - `path_in_repo`: der Repository-Pfad, wohin die Datei kopiert werden soll.
  - `src_revision`: optional - die Revision der zu kopierenden Datei, wenn Sie eine Datei von einem anderen Branch/Revision kopieren m√∂chten.

Zum Beispiel, wenn Sie zwei Dateien hochladen und eine Datei in einem Hub-Repository l√∂schen m√∂chten:

1. Verwenden Sie die entsprechende `CommitOperation`, um eine Datei hinzuzuf√ºgen oder zu l√∂schen und einen Ordner zu l√∂schen:

```py
>>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete
>>> api = HfApi()
>>> operations = [
...     CommitOperationAdd(path_in_repo="LICENSE.md", path_or_fileobj="~/repo/LICENSE.md"),
...     CommitOperationAdd(path_in_repo="weights.h5", path_or_fileobj="~/repo/weights-final.h5"),
...     CommitOperationDelete(path_in_repo="old-weights.h5"),
...     CommitOperationDelete(path_in_repo="logs/"),
...     CommitOperationCopy(src_path_in_repo="image.png", path_in_repo="duplicate_image.png"),
... ]
```

2. √úbergeben Sie Ihre Operationen an [`create_commit`]:

```py
>>> api.create_commit(
...     repo_id="lysandre/test-model",
...     operations=operations,
...     commit_message="Hochladen meiner Modell-Gewichte und -Lizenz",
... )
```

Zus√§tzlich zu [`upload_file`] und [`upload_folder`] verwenden auch die folgenden Funktionen [`create_commit`] im Hintergrund:

- [`delete_file`] l√∂scht eine einzelne Datei aus einem Repository auf dem Hub.
- [`delete_folder`] l√∂scht einen gesamten Ordner aus einem Repository auf dem Hub.
- [`metadata_update`] aktualisiert die Metadaten eines Repositorys.

F√ºr detailliertere Informationen werfen Sie einen Blick auf die [`HfApi`] Referenz.

## Tipps und Tricks f√ºr gro√üe Uploads

Bei der Verwaltung einer gro√üen Datenmenge in Ihrem Repo gibt es einige Einschr√§nkungen zu beachten. Angesichts der Zeit, die es dauert, die Daten zu streamen, kann es sehr √§rgerlich sein, am Ende des Prozesses einen Upload/Push zu verlieren oder eine degradierte Erfahrung zu machen, sei es auf hf.co oder bei lokalem Arbeiten. Wir haben eine Liste von Tipps und Empfehlungen zusammengestellt, um Ihr Repo zu strukturieren.

| Eigenschaft     | Empfohlen        | Tipps                                                  |
| ----------------   | ------------------ | ------------------------------------------------------ |
| Repo-Gr√∂√üe         | -                  | Kontaktieren Sie uns f√ºr gro√üe Repos (TBs Daten)               |
| Dateien pro Repo     | <100k              | Daten in weniger Dateien zusammenf√ºhren                            |
| Eintr√§ge pro Ordner | <10k               | Unterverzeichnisse im Repo verwenden                             |
| Dateigr√∂√üe         | <5GB               | Daten in geteilte Dateien aufteilen                          |
| Commit-Gr√∂√üe        | <100 files*        | Dateien in mehreren Commits hochladen                       |
| Commits pro Repo   | -                  | Mehrere Dateien pro Commit hochladen und/oder Historie zusammenf√ºhren |

_* Nicht relevant bei direkter Verwendung des `git` CLI_

Bitte lesen Sie den n√§chsten Abschnitt, um diese Beschr√§nkungen besser zu verstehen und zu erfahren, wie Sie damit umgehen k√∂nnen.

### Hub-Repository Gr√∂√üenbeschr√§nkungen

Was meinen wir, wenn wir von "gro√üen Uploads" sprechen, und welche Einschr√§nkungen sind damit verbunden? Gro√üe Uploads k√∂nnen sehr unterschiedlich sein, von Repositories mit einigen riesigen Dateien (z. B. Modellgewichten) bis hin zu Repositories mit Tausenden von kleinen Dateien (z. B. einem Bilddatensatz).

Hinter den Kulissen verwendet der Hub Git zur Versionierung der Daten, was strukturelle Auswirkungen darauf hat, was Sie in Ihrem Repo tun k√∂nnen.
Wenn Ihr Repo einige der im vorherigen Abschnitt erw√§hnten Zahlen √ºberschreitet, **empfehlen wir Ihnen dringend, [`git-sizer`](https://github.com/github/git-sizer) zu verwenden**, das eine sehr detaillierte Dokumentation √ºber die verschiedenen Faktoren bietet, die Ihr Erlebnis beeinflussen werden. Hier ist ein TL;DR der zu ber√ºcksichtigenden Faktoren:

- **Repository-Gr√∂√üe**: Die Gesamtgr√∂√üe der Daten, die Sie hochladen m√∂chten. Es gibt keine feste Obergrenze f√ºr die Gr√∂√üe eines Hub-Repositories. Wenn Sie jedoch vorhaben, Hunderte von GBs oder sogar TBs an Daten hochzuladen, w√ºrden wir es begr√º√üen, wenn Sie uns dies im Voraus mitteilen k√∂nnten, damit wir Ihnen besser helfen k√∂nnen, falls Sie w√§hrend des Prozesses Fragen haben. Sie k√∂nnen uns unter datasets@huggingface.co oder auf [unserem Discord](http://hf.co/join/discord) kontaktieren.
- **Anzahl der Dateien**:
    - F√ºr ein optimales Erlebnis empfehlen wir, die Gesamtzahl der Dateien unter 100k zu halten. Versuchen Sie, die Daten zu weniger Dateien zusammenzuf√ºhren, wenn Sie mehr haben.
      Zum Beispiel k√∂nnen json-Dateien zu einer einzigen jsonl-Datei zusammengef√ºhrt oder gro√üe Datens√§tze als Parquet-Dateien exportiert werden.
    - Die maximale Anzahl von Dateien pro Ordner darf 10k Dateien pro Ordner nicht √ºberschreiten. Eine einfache L√∂sung besteht darin, eine Repository-Struktur zu erstellen, die Unterverzeichnisse verwendet. Ein Repo mit 1k Ordnern von `000/` bis `999/`, in dem jeweils maximal 1000 Dateien enthalten sind, reicht bereits aus.
- **Dateigr√∂√üe**: Bei hochzuladenden gro√üen Dateien (z. B. Modellgewichte) empfehlen wir dringend, sie **in Bl√∂cke von etwa 5GB aufzuteilen**.
Es gibt mehrere Gr√ºnde daf√ºr:
    - Das Hoch- und Herunterladen kleinerer Dateien ist sowohl f√ºr Sie als auch f√ºr andere Benutzer viel einfacher. Bei der Daten√ºbertragung k√∂nnen immer Verbindungsprobleme auftreten, und kleinere Dateien vermeiden das erneute Starten von Anfang an im Falle von Fehlern.
    - Dateien werden den Benutzern √ºber CloudFront bereitgestellt. Aus unserer Erfahrung werden riesige Dateien von diesem Dienst nicht zwischengespeichert, was zu einer langsameren Downloadgeschwindigkeit f√ºhrt.
In jedem Fall wird keine einzelne LFS-Datei >50GB sein k√∂nnen. D. h. 50GB ist das absolute Limit f√ºr die Einzeldateigr√∂√üe.
- **Anzahl der Commits**: Es gibt kein festes Limit f√ºr die Gesamtzahl der Commits in Ihrer Repo-Historie. Aus unserer Erfahrung heraus beginnt das Benutzererlebnis im Hub jedoch nach einigen Tausend Commits abzunehmen. Wir arbeiten st√§ndig daran, den Service zu verbessern, aber man sollte immer daran denken, dass ein Git-Repository nicht als Datenbank mit vielen Schreibzugriffen gedacht ist. Wenn die Historie Ihres Repos sehr gro√ü wird, k√∂nnen Sie immer alle Commits mit [`super_squash_history`] zusammenfassen, um einen Neuanfang zu erhalten. Dies ist eine nicht r√ºckg√§ngig zu machende Operation.
- **Anzahl der Operationen pro Commit**: Auch hier gibt es keine feste Obergrenze. Wenn ein Commit im Hub hochgeladen wird, wird jede Git-Operation (Hinzuf√ºgen oder L√∂schen) vom Server √ºberpr√ºft. Wenn hundert LFS-Dateien auf einmal committed werden, wird jede Datei einzeln √ºberpr√ºft, um sicherzustellen, dass sie korrekt hochgeladen wurde. Beim Pushen von Daten √ºber HTTP mit `huggingface_hub` wird ein Timeout von 60s f√ºr die Anforderung festgelegt, was bedeutet, dass, wenn der Prozess mehr Zeit in Anspruch nimmt, clientseitig ein Fehler ausgel√∂st wird. Es kann jedoch (in seltenen F√§llen) vorkommen, dass selbst wenn das Timeout clientseitig ausgel√∂st wird, der Prozess serverseitig dennoch abgeschlossen wird. Dies kann manuell √ºberpr√ºft werden, indem man das Repo im Hub durchsucht. Um dieses Timeout zu vermeiden, empfehlen wir, pro Commit etwa 50-100 Dateien hinzuzuf√ºgen.

### Praktische Tipps

Nachdem wir die technischen Aspekte gesehen haben, die Sie bei der Strukturierung Ihres Repositories ber√ºcksichtigen m√ºssen, schauen wir uns einige praktische Tipps an, um Ihren Upload-Prozess so reibungslos wie m√∂glich zu gestalten.

- **Fangen Sie klein an**: Wir empfehlen, mit einer kleinen Datenmenge zu beginnen, um Ihr Upload-Skript zu testen. Es ist einfacher, an einem Skript zu arbeiten, wenn ein Fehler nur wenig Zeit kostet.
- **Rechnen Sie mit Ausf√§llen**: Das Streamen gro√üer Datenmengen ist eine Herausforderung. Sie wissen nicht, was passieren kann, aber es ist immer am besten anzunehmen, dass etwas mindestens einmal schiefgehen wird - unabh√§ngig davon, ob es an Ihrem Ger√§t, Ihrer Verbindung oder unseren Servern liegt. Wenn Sie zum Beispiel vorhaben, eine gro√üe Anzahl von Dateien hochzuladen, ist es am besten, lokal zu verfolgen, welche Dateien Sie bereits hochgeladen haben, bevor Sie die n√§chste Batch hochladen. Sie k√∂nnen sicher sein, dass eine LFS-Datei, die bereits committed wurde, niemals zweimal hochgeladen wird, aber es kann clientseitig trotzdem Zeit sparen, dies zu √ºberpr√ºfen.
- **Verwenden Sie `hf_transfer`**: Dabei handelt es sich um eine auf Rust basierende [Bibliothek](https://github.com/huggingface/hf_transfer), die dazu dient, Uploads auf Maschinen mit sehr hoher Bandbreite zu beschleunigen. Um sie zu verwenden, m√ºssen Sie sie installieren (`pip install hf_transfer`) und sie durch Einstellen von `HF_HUB_ENABLE_HF_TRANSFER=1` als Umgebungsvariable aktivieren. Anschlie√üend k√∂nnen Sie `huggingface_hub` wie gewohnt verwenden.
Hinweis: Dies ist ein Tool f√ºr Power-User. Es ist getestet und einsatzbereit, verf√ºgt jedoch nicht √ºber benutzerfreundliche Funktionen wie Fortschrittsanzeigen oder erweiterte Fehlerbehandlung.

## (veraltet) Dateien mit Git LFS hochladen

Alle oben beschriebenen Methoden verwenden die Hub-API, um Dateien hochzuladen. Dies ist der empfohlene Weg, Dateien in den Hub hochzuladen.
Wir bieten jedoch auch [`Repository`] an, einen Wrapper um das git-Tool, um ein lokales Repository zu verwalten.

> [!WARNING]
> Obwohl [`Repository`] formell nicht als veraltet gekennzeichnet ist, empfehlen wir stattdessen die Nutzung der HTTP-basierten Methoden, die oben beschrieben sind.
> F√ºr weitere Details zu dieser Empfehlung werfen Sie bitte einen Blick auf diesen [Leitfaden](../concepts/git_vs_http), der die Kernunterschiede zwischen HTTP- und Git-basierten Ans√§tzen erkl√§rt.

Git LFS verarbeitet automatisch Dateien, die gr√∂√üer als 10MB sind. F√ºr sehr gro√üe Dateien (>5GB) m√ºssen Sie jedoch einen benutzerdefinierten Transferagenten f√ºr Git LFS installieren:

```bash
hf lfs-enable-largefiles .
```

Sie sollten dies f√ºr jedes Repository installieren, das eine sehr gro√üe Datei enth√§lt. Einmal installiert, k√∂nnen Sie Dateien hochladen, die gr√∂√üer als 5GB sind.

### commit Kontextmanager

Der `commit` Kontextmanager handhabt vier der g√§ngigsten Git-Befehle: pull, add, commit und push. `git-lfs` beobactet automatisch jede Datei, die gr√∂√üer als 10MB ist. Im folgenden Beispiel handhabt der `commit` Kontextmanager die folgenden Aufgaben:

1. Holt Daten aus dem `text-files` Repository.
2. F√ºgt eine √Ñnderung an `file.txt` hinzu.
3. Committet die √Ñnderung.
4. Schickt die √Ñnderung an das `text-files` Repository.

```python
>>> from huggingface_hub import Repository
>>> with Repository(local_dir="text-files", clone_from="<user>/text-files").commit(commit_message="Mein erste Datei :)"):
...     with open("file.txt", "w+") as f:
...         f.write(json.dumps({"hey": 8}))
```

Hier ist ein weiteres Beispiel, wie man den `commit` Kontextmanager verwendet, um eine Datei in einem Repository zu speichern und hochzuladen:

```python
>>> import torch
>>> model = torch.nn.Transformer()
>>> with Repository("torch-model", clone_from="<user>/torch-model", token=True).commit(commit_message="Mein cooles Model :)"):
...     torch.save(model.state_dict(), "model.pt")
```

Setzen Sie `blocking=False`, wenn Sie Ihre Commits asynchron pushen m√∂chten. Das nicht-blockierende Verhalten ist n√ºtzlich, wenn Sie Ihr Skript weiterhin ausf√ºhren m√∂chten, w√§hrend Ihre Commits gesendet werden.

```python
>>> with repo.commit(commit_message="Mein cooles Model :)", blocking=False)
```

Sie k√∂nnen den Status Ihres Pushs mit der Methode `command_queue` √ºberpr√ºfen:

```python
>>> last_command = repo.command_queue[-1]
>>> last_command.status
```

Beachten Sie die Tabelle mit m√∂glichen Statuscodes:

| Status   | Beschreibung                         |
| -------- | ------------------------------------ |
| -1       | Der Push wird ausgef√ºhrt.            |
| 0        | Der Push wurde erfolgreich beendet.  |
| Non-zero | Ein Fehler ist aufgetreten.          |

Wenn `blocking=False` gesetzt ist, werden Befehle beobachtet und Ihr Skript wird erst beendet, wenn alle Pushs abgeschlossen sind, auch wenn andere Fehler in Ihrem Skript auftreten. Einige zus√§tzliche n√ºtzliche Befehle, um den Status eines Pushs zu √ºberpr√ºfen, sind:

```python
# Einen Fehler inspizieren.
>>> last_command.stderr

# √úberpr√ºfen, ob ein Push abgeschlossen ist oder noch l√§uft.
>>> last_command.is_done

# √úberpr√ºfen, ob bei einem Push-Befehl ein Fehler aufgetreten ist.
>>> last_command.failed
```

### push_to_hub

Die Klasse [`Repository`] hat eine Funktion [`~Repository.push_to_hub`], um Dateien hinzuzuf√ºgen, einen Commit zu machen und diese zu einem Repository zu pushen. Im Gegensatz zum `commit` Kontextmanager m√ºssen Sie zuerst von einem Repository pullen, bevor Sie [`~Repository.push_to_hub`] aufrufen.

Zum Beispiel, wenn Sie bereits ein Repository vom Hub geklont haben, k√∂nnen Sie das `repo` vom lokalen Verzeichnis initialisieren:

```python
>>> from huggingface_hub import Repository
>>> repo = Repository(local_dir="pfad/zur/lokalen/repo")
```
Aktualisieren Sie Ihren lokalen Klon mit [`~Repository.git_pull`] und dann pushen Sie Ihre Datei zum Hub:

```py
>>> repo.git_pull()
>>> repo.push_to_hub(commit_message="Committe meine geniale Datei zum Hub")
```

Wenn Sie jedoch noch nicht bereit sind, eine Datei zu pushen, k√∂nnen Sie [`~Repository.git_add`] und [`~Repository.git_commit`] verwenden, um nur Ihre Datei hinzuzuf√ºgen und zu committen:

```py
>>> repo.git_add("path/to/file")
>>> repo.git_commit(commit_message="f√ºge meine erste Modell-Konfigurationsdatei hinzu :)")
```

Wenn Sie bereit sind, pushen Sie die Datei zu Ihrem Repository mit [`~Repository.git_push`]:

```py
>>> repo.git_push()
```
