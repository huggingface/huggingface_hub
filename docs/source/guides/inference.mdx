# Run Inference on servers

Inference is the process of using a trained model to make predictions on new data. This process can be compute-intensive
which makes it interesting to run on a dedicated server. The `huggingface_hub` library provides an easy way to call a
service that runs inference for hosted models. There are several services you can connect to:
- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference
on Hugging Face's infrastructure and for free. This service is a fast way to get started, tests different models and
prototype AI products.
- [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production.
Inference is run by Hugging Face in a dedicated, fully managed infrastructure on the Cloud of your choice.
- [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)

All of these services can be called with the [`InferenceClient`] object. Let's see how to use it!

<Tip>

[`InferenceClient`] is a Python client making HTTP calls to our APIs. If you want to make the HTTP calls directly, please
refer to the [Inference API](https://huggingface.co/docs/api-inference/index) or to the
[Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) documentation pages.

For web development, a [JS client](https://huggingface.co/docs/huggingface.js/inference/README) has been released.
If you are interested in game development, you might have a look to our [C# project](https://github.com/huggingface/unity-api).

</Tip>

<Tip>

[`InferenceClient`] acts as a replacement for the legacy [`InferenceApi`] client. It adds specific support for tasks and
handles inference on both [Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).
Learn how to migrate to the new client in this section.

</Tip>

## Getting started

Let's get started with a first example:

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient()

>>> image = client.text_to_image("An astronaut riding a horse on the moon.")
>>> image.save("astronaut.png")
```

We initialized an [`InferenceClient`] with the default parameters. The only thing you need to know is the task you want
to perform. By default, the client will connect to the Inference API and select a model to complete the task. In our
example, we generated an image from a text prompt. The returned value is a `PIL.Image` object that can be saved to a
file.

### Using a specific model

What if you want to use a specific model? You can specify it either as a parameter or directly at an instance level:

```python
>>> from huggingface_hub import InferenceClient
# Initialize client for a specific model
>>> client = InferenceClient(model="prompthero/openjourney-v4")
>>> client.text_to_image(...)
# Or use a generic client but pass your model as an argument 
>>> client = InferenceClient()
>>> client.text_to_image(..., model="prompthero/openjourney-v4")
```

<Tip>

There are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceClient`] comes with a recommended
model. However in most cases you'll be interested in finding a model specific to _your_ needs. Visit the
https://huggingface.co/models page to explore your possibilities. 

</Tip>

### Using a specific URL

The examples we saw above are using the free-hosted Inference API. This proves to be very useful to quickly prototype
and test thins. Once you're ready to deploy your model to production, you'll need to use a dedicated infrastructure.
That's were [Inference Endpoints](https://huggingface.co/inference-endpoints) comes into play. It allows you to deploy
any model and expose it as a private API. Once deployed, you'll get a URL that you can connect to using exactly the same
code as before, changing only the `model` parameter:

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient(model="https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if")
# or 
>>> client = InferenceClient()
>>> client.text_to_image(..., model="https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if")
```

### Authentication

Calls made with the [`InferenceClient`] can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens).
By default, it will use the token saved on your machine if you are logged in (check out
[how to login](https://huggingface.co/docs/huggingface_hub/quick-start#login)). If you are not logged in, you can pass
your token as an instance parameter:

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient(token="hf_***")
```

<Tip>

Authentication is NOT mandatory when using the Inference API. However, authenticated users gets a higher free-tier to
play with the service. Token is also mandatory if you want to run inference on your private models or on private
endpoints.

</Tip>

## Supported tasks

[`InferenceClient`]'s goal is to provide the most easy-to-use interface to run inference on Hugging Face models. It
has a simple API that supports the most common tasks. Here is a list of the currently supported tasks:

- audio
    - [x] [`~InferenceClient.audio_classification`]
    - [x] [`~InferenceClient.automatic_speech_recognition`]
    - [x] [`~InferenceClient.text_to_speech`]
- computer vision
    - [x] [`~InferenceClient.image_classification`]
    - [x] [`~InferenceClient.image_segmentation`]
    - [x] [`~InferenceClient.image_to_image`]
    - [x] [`~InferenceClient.image_to_text`]
    - [ ] object detection
    - [x] [`~InferenceClient.text_to_image`]
- multimodal
    - [ ] documentation question answering
    - [ ] visual question answering
- nlp
    - [x] [`~InferenceClient.conversational`]
    - [x] [`~InferenceClient.feature_extraction`]
    - [ ] fill mask
    - [ ] question answering
    - [x] [`~InferenceClient.sentence_similarity`]
    - [x] [`~InferenceClient.summarization`]
    - [ ] table question answering
    - [ ] text classification
    - [ ] text generation
    - [ ] token classification
    - [ ] translation
    - [ ] zero shot classification
- tabular
    - [ ] tabular classification
    - [ ] tabular regression

This list is meant to be completed to support all tasks. If you are particularly interested in a task not yet supported,
please let us know on [huggingface_hub's repo](https://github.com/huggingface/huggingface_hub).

<Tip>

Check out the tasks page (https://huggingface.co/tasks) to learn more about each task, how to use them and what are the
most popular models for each of them.

</Tip>

<Tip warning={true}>

The API is designed to be simple. Not all parameters and options are available or described for the end user. Check out
[this page](https://huggingface.co/docs/api-inference/detailed_parameters) if you are interested in learning more about
all the parameters available for each task.

</Tip>

## Custom requests

Despite the simplicity of the [`InferenceClient`] API, it is not always possible to cover all use cases. The
[`InferenceClient.post`] method allows you to send any request to the Inference API. With this flexibility, you can
for example handle yourself the parsing of the inputs and output. 

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient()
>>> response = client.post(json={"inputs": "An astronaut riding a horse on the moon."}, model="stabilityai/stable-diffusion-2-1")
>>> response.content # raw bytes
b'...'
```

In the example above, the generated image is returned as raw bytes. This can be helpful if you don't have `Pillow`
installed in your setup and just care about the binary content of the image. [`InferenceClient.post`] is also useful
to handle tasks that are not yet officially supported.

## Advanced tips

In the above section, we saw the main aspects of [`InferenceClient`]. Let's dive into some more advanced tips.

### Timeout

When doing inference, there are two main causes for a timeout to happen:
- The inference process takes a long time to complete.
- The model is not available, for example when Inference API is loading it for the first time.

[`InferenceClient`] has a global `timeout` parameter to handle those two aspects. By default, it is set to `None`,
meaning that the client will wait indefinitely for the inference to complete. If you want more control in your workflow,
you can set it to a specific value in seconds. If the timeout delay expires, an [`InferenceTimeoutError`] is raised.
You can catch it and handle it in your code:

```python
>>> from huggingface_hub import InferenceClient, InferenceTimeoutError
>>> client = InferenceClient(timeout=30)
>>> try:
...     client.text_to_image(...)
... except InferenceTimeoutError:
...     print("Inference timed out after 30s.")
```

### Binary inputs

Some tasks require binary inputs, for example when dealing with images or audio files. In this case, [`InferenceClient`]
tries to be as permissive as possible and accept different types:
- raw `bytes`
- a file-like object, opened as binary (`with open("audio.wav", "rb") as f: ...`)
- a path (`str` or `Path`) pointing to a local file
- a URL (`str`) pointing to a remote file (e.g. `https://...`). In this case, the file will be downloaded locally before
sending it to the Inference API.

```py
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient()
>>> client.image_classification("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg")
[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]
```

## Legacy InferenceAPI client

[`InferenceClient`] acts as a replacement for the legacy [`InferenceApi`] client. It adds specific support for tasks and
handles inference on both [Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).

Here is a short guide to help you migrate from [`InferenceApi`] to [`InferenceClient`].

### Initialization

Change from 

```python
>>> from huggingface_hub import InferenceApi
>>> inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)
```

to

```python
>>> from huggingface_hub import InferenceClient
>>> inference = InferenceClient(model="bert-base-uncased", token=API_TOKEN)
```

### Run on a specific task

Change from 

```python
>>> from huggingface_hub import InferenceApi
>>> inference = InferenceApi(repo_id="paraphrase-xlm-r-multilingual-v1", task="feature-extraction")
>>> inference(...)
```

to

```python
>>> from huggingface_hub import InferenceClient
>>> inference = InferenceClient()
>>> inference.feature_extraction(..., model="paraphrase-xlm-r-multilingual-v1")
```

<Tip>

This is the recommended way to adapt your code to [`InferenceClient`]. It lets you benefit from the task-specific
methods like `feature_extraction`.

</Tip>

### Run custom request

Change from

```python
>>> from huggingface_hub import InferenceApi
>>> inference = InferenceApi(repo_id="bert-base-uncased")
>>> inference(inputs="The goal of life is [MASK].")
[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]
```

to

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient()
>>> response = client.post(json={"inputs": "The goal of life is [MASK]."}, model="bert-base-uncased")
>>> response.json()
[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]
```

### Run with parameters

Change from

```python
>>> from huggingface_hub import InferenceApi
>>> inference = InferenceApi(repo_id="typeform/distilbert-base-uncased-mnli")
>>> inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
>>> params = {"candidate_labels":["refund", "legal", "faq"]}
>>> inference(inputs, params)
{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}
```

to

```python
>>> from huggingface_hub import InferenceClient
>>> client = InferenceClient()
>>> inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
>>> params = {"candidate_labels":["refund", "legal", "faq"]}
>>> response = client.post(json={"inputs": inputs, "parameters": params}, model="typeform/distilbert-base-uncased-mnli")
>>> response.json()
{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}
```